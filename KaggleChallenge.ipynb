{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/dev/perso/Tweet_Prediction/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">1. Preprocessing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:indigo\"> 1.1 Dataset observation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecture du fichier .csv par la libraire Pandas\n",
    "data=pd.read_csv('train.csv')\n",
    "df1=data.copy()\n",
    "df_test1=pd.read_csv('evaluation.csv')\n",
    "\n",
    "#We need to separate X and Y\n",
    "#X=data.loc[:,data.columns!=\"retweets_count\"]\n",
    "#Y=data['retweets_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">1.2 Features explanations </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.quantile(0.99) #We check if there is a lot ludicrous values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#If our dataset contains empty values, we put the mean value of the collum\n",
    "if df.isnull().sum().sum()!=0: \n",
    "    imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "else: \n",
    "    print(\"No missing_values in this dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(28,24))\n",
    "sns.heatmap(data = corr_matrix,cmap='BrBG', annot=True, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------> We observe that the only strong correlation that we can see at this time is the one between [favorite_count] \n",
    "and [retweet_count] , so we can already say that to predict the number of retweet, the number of favorite count is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#checking for missing values in output\n",
    "for i in range(df.shape[0]):\n",
    "    if df['retweets_count'][i]==[]:\n",
    "        print(df['retweets_count'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">1.3 Normalisations </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit normaliser les données mais chaque colonne doit être normalisé différemment. \n",
    "- \"favorites_count\" -> moins de 1% de valeurs abhérantes (au top)\n",
    "- \"followers_count\", -> Exponential \n",
    "- \"statutes_count\",\n",
    "- \"friends_count\" -> moins de 1% de valeurs abhérantes (au top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization.quantile(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_into_df(df):\n",
    "    result=pd.DataFrame(df,columns=[\"month\", \"day\", \"moment\"])\n",
    "    timestamps=np.array(df[\"timestamp\"])\n",
    "    \n",
    "    for i in range(timestamps.shape[0]):\n",
    "        date=dt.fromtimestamp(timestamps[i]/1000)\n",
    "        result[\"month\"][i]=date.month\n",
    "        result[\"day\"][i]=date.day\n",
    "        result[\"moment\"][i]=date.hour+date.minute/60\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x):\n",
    "     return np.log(1+x)\n",
    "\n",
    "def Normaliser(df, df_test):\n",
    "    \n",
    "    column_names=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"]\n",
    "    \n",
    "    #Create DFs and apply log to them\n",
    "    \n",
    "    Norm_train=pd.DataFrame(df,columns=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"])\n",
    "    Norm_train['favorites_count'] = Norm_train['favorites_count'].transform(myfunc)\n",
    "    Norm_train['followers_count'] = Norm_train['followers_count'].transform(myfunc)\n",
    "    Norm_train['friends_count'] = Norm_train['friends_count'].transform(myfunc)\n",
    "    Norm_train['statuses_count'] = Norm_train['statuses_count'].transform(myfunc)\n",
    "    \n",
    "    Norm_rt=pd.DataFrame(df,columns=[\"retweets_count\"])\n",
    "    Norm_rt['retweets_count'] = Norm_rt['retweets_count'].transform(myfunc)\n",
    "    \n",
    "    Norm_test=pd.DataFrame(df_test,columns=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"])\n",
    "    Norm_test['favorites_count'] = Norm_test['favorites_count'].transform(myfunc)\n",
    "    Norm_test['followers_count'] = Norm_test['followers_count'].transform(myfunc)\n",
    "    Norm_test['friends_count'] = Norm_test['friends_count'].transform(myfunc)\n",
    "    Norm_test['statuses_count'] = Norm_test['statuses_count'].transform(myfunc)\n",
    "    \n",
    "    #Also add time\n",
    "    \n",
    "    Time_train=get_time_into_df(df)\n",
    "    Time_test=get_time_into_df(df_test)\n",
    "    \n",
    "    #Now rescale with min-max\n",
    "    \n",
    "    scaler_train=MinMaxScaler()\n",
    "    scaler_train.fit(Norm_train)\n",
    "    norm_train=scaler_train.transform(Norm_train)\n",
    "    norm_test=scaler_train.transform(Norm_test)\n",
    "    \n",
    "    scaler_rt=MinMaxScaler()\n",
    "    scaler_rt.fit(Norm_rt)\n",
    "    norm_rt=scaler_rt.transform(Norm_rt)\n",
    "    \n",
    "    scaler_time=MinMaxScaler()\n",
    "    scaler_time.fit(Time_train)\n",
    "    norm_time_train=scaler_time.transform(Time_train)\n",
    "    norm_time_test=scaler_time.transform(Time_test)\n",
    "    \n",
    "    #Now put the rescaled results into the original DFs\n",
    "    \n",
    "    Norm_train=pd.DataFrame(norm_train,columns=column_names)\n",
    "    Norm_test=pd.DataFrame(norm_test,columns=column_names)\n",
    "    Norm_rt=pd.DataFrame(norm_rt,columns=[\"retweets_count\"])\n",
    "    \n",
    "    NormT_train=pd.DataFrame(norm_time_train,columns=[\"month\", \"day\", \"moment\"])\n",
    "    NormT_test=pd.DataFrame(norm_time_test,columns=[\"month\", \"day\", \"moment\"])\n",
    "    \n",
    "    df['favorites_count']=Norm_train['favorites_count']\n",
    "    df['followers_count']=Norm_train['followers_count'] \n",
    "    df['statuses_count']=Norm_train['statuses_count']\n",
    "    df['friends_count']=Norm_train['friends_count']\n",
    "    df['retweets_count']=Norm_rt['retweets_count']\n",
    "    \n",
    "    df_test['favorites_count']=Norm_test['favorites_count']\n",
    "    df_test['followers_count']=Norm_test['followers_count'] \n",
    "    df_test['statuses_count']=Norm_test['statuses_count']\n",
    "    df_test['friends_count']=Norm_test['friends_count']\n",
    "    \n",
    "    df[\"month\"]=NormT_train[\"month\"]\n",
    "    df[\"day\"]=NormT_train[\"day\"]\n",
    "    df[\"moment\"]=NormT_train[\"moment\"]\n",
    "    \n",
    "    df_test[\"month\"]=NormT_test[\"month\"]\n",
    "    df_test[\"day\"]=NormT_test[\"day\"]\n",
    "    df_test[\"moment\"]=NormT_test[\"moment\"]\n",
    "\n",
    "    return df, df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test= Normaliser(df1,df_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">2. PCA Text and # treatment </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:indigo\">2.1 Preprocessing of hashtags and texts </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for hashtags\n",
    "\n",
    "def most_important_hashtags(df, criteria):\n",
    "    DICO2={} #DICO2 is for hashtags\n",
    "    I=df.shape[0]\n",
    "    for i in range(I):\n",
    "        sentence=df[\"hashtags\"][i][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in DICO2:\n",
    "                DICO2[extracted_word]+=1\n",
    "            else:\n",
    "                DICO2[extracted_word]=1\n",
    "    most_occurr_DICO2={}\n",
    "    for word in DICO2:\n",
    "        if DICO2[word]>criteria :#and word!=''\n",
    "            most_occurr_DICO2[word]=0\n",
    "    \n",
    "    important_hashtags=list(most_occurr_DICO2.keys())\n",
    "    \n",
    "    measurer2={}\n",
    "    for i in range(len(important_hashtags)):\n",
    "        measurer2[important_hashtags[i]]=i\n",
    "    measurer2\n",
    "    return important_hashtags, measurer2\n",
    "\n",
    "def make_array_training_hashtags(df, criteria):\n",
    "    possibilities, measurer=most_important_hashtags(df, criteria)\n",
    "    result=np.zeros((df.shape[0], len(possibilities)))\n",
    "    for line in range(df.shape[0]):\n",
    "        sentence=df[\"hashtags\"][line][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in possibilities:\n",
    "                result[line,measurer[extracted_word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "def make_array_test_hashtags(df_test, criteria):\n",
    "    possibilities, measurer=most_important_hashtags(df, criteria)\n",
    "    result=np.zeros((df_test.shape[0], len(possibilities)))\n",
    "    for line in range(df_test.shape[0]):\n",
    "        sentence=df_test[\"hashtags\"][line][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in possibilities:\n",
    "                result[line,measurer[extracted_word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "array_train_hashtags=make_array_training_hashtags(df,10)\n",
    "array_test_hashtags=make_array_test_hashtags(df_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for texts\n",
    "\n",
    "def most_important_words(df, criteria):\n",
    "    DICO={}\n",
    "    I=df.shape[0]\n",
    "    for i in range(I):\n",
    "        sentence=df[\"text\"][i].split()\n",
    "        for word in sentence:\n",
    "            if word in DICO:\n",
    "                DICO[word]+=1\n",
    "            else:\n",
    "                DICO[word]=1\n",
    "            \n",
    "    most_occurr_DICO={}\n",
    "    for word in DICO:\n",
    "        if DICO[word]>criteria:\n",
    "            most_occurr_DICO[word]=0\n",
    "    \n",
    "    important_words=list(most_occurr_DICO.keys())\n",
    "    measurer={}\n",
    "    for i in range(len(important_words)):\n",
    "        measurer[important_words[i]]=i\n",
    "    measurer\n",
    "    \n",
    "    return important_words, measurer\n",
    "\n",
    "def make_array_training_words(df, criteria):\n",
    "    possibilities, measurer=most_important_words(df, criteria)\n",
    "    result=np.zeros((df.shape[0], len(possibilities)))\n",
    "    for line in range(df.shape[0]):\n",
    "        sentence=df[\"text\"][line].split()\n",
    "        for word in sentence:\n",
    "            if word in possibilities:\n",
    "                result[line,measurer[word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "def make_array_test_words(df_test, criteria):\n",
    "    possibilities, measurer=most_important_words(df, criteria)\n",
    "    result=np.zeros((df_test.shape[0], len(possibilities)))\n",
    "    for line in range(df_test.shape[0]):\n",
    "        sentence=df_test[\"text\"][line].split()\n",
    "        for word in sentence:\n",
    "            if word in possibilities:\n",
    "                result[line,measurer[word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "array_train_text=make_array_training_words(df,5000)\n",
    "array_test_text=make_array_test_words(df_test, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "## <span style=\"color:indigo\">2.2 PCA on words and hashtags: Understanding the main components of bags of words </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_training(array, n_comp):\n",
    "    n=array.shape[1]\n",
    "    for col in range(n):\n",
    "        mean=np.mean(array[:,col])\n",
    "        array[:,col]-=mean\n",
    "            \n",
    "    C=np.matmul(array.T,array)\n",
    "    C/=array.shape[1]\n",
    "    \n",
    "    Lambda, Q=np.linalg.eigh(C)\n",
    "\n",
    "    Qk=np.zeros((n,n_comp))\n",
    "    for col in range(n_comp):\n",
    "        Qk[:,col]=Q[:,n-col-1]\n",
    "    return np.matmul(array,Qk), Qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_test(training_array, test_array, n_comp):\n",
    "    return np.matmul(test_array,PCA_training(training_array,n_comp)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_TRAIN_TEXT=PCA_training(array_train_text, 4)\n",
    "PCA_TRAIN_HASHTAGS=PCA_training(array_train_hashtags, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">3. Preparing RNN on word sequences: Understanding relations between words </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:indigo\"> 3.1 Pre-treatment for word embedding </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the FastText method. We divide each word into groups of 3 letters (spaces are the \"empty letter\"). We keep the order in the text and make an embedding also using the adjacent words. For instance:\n",
    "\"macron demission\"==> -ma,mac,acr,...,on-,n-d,-de,dem,... (- is the empty letter)\n",
    "\n",
    "The fast text method will guess the adjacent 3 letters to each given patch of 3 letters. Hence for \"mac\" we want a high probability of \"---\" (3 times empty letter) and \"ron\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\"  \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns an array with the words in the sentence cut (n=3 that is in pieces of 3)\n",
    "\n",
    "def cutter(sent, n):\n",
    "    sentence=sent.split()\n",
    "    length=len(sentence)+2-n\n",
    "    for word in sentence:\n",
    "        length+=len(word)\n",
    "    result=[]\n",
    "    padding=''\n",
    "    for i in range(n):\n",
    "        padding+=' '\n",
    "    sent=' '+sent+' '\n",
    "    padd_sent=padding+sent+padding\n",
    "    \n",
    "    for i in range(length):\n",
    "        middle=sent[i:i+n]\n",
    "        before=padd_sent[i:i+n]\n",
    "        after=padd_sent[2*n+i:3*n+i]\n",
    "        result.append([before, middle, after])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphabet=30\n",
    "\n",
    "def batch_maker(size):\n",
    "    X=np.zeros((size,3*n_alphabet))\n",
    "    Y=np.zeros((size,6*n_alphabet))\n",
    "    for i in range(size):\n",
    "        \n",
    "        line=random.randrange(0,350000)\n",
    "        cut=cutter(df[\"text\"][line],3)\n",
    "        \n",
    "        alphabet={\n",
    "        ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "        'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "        'è':28,\n",
    "        #thus use mod 30 (as 29 is for any other letter (arabic...))\n",
    "        }\n",
    "        \n",
    "        possibilities=[        \n",
    "        ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "        'y','z','é','è',]\n",
    "        \n",
    "        length=cut.shape[0]\n",
    "        word_index=random.randrange(0,length)\n",
    "        \n",
    "        word0=cut[word_index, 0]\n",
    "        word1=cut[word_index, 1]\n",
    "        word2=cut[word_index, 2]\n",
    "        for letter_ind in range(3):\n",
    "            index0=0\n",
    "            index1=0\n",
    "            index2=0\n",
    "            if (word0[letter_ind] not in possibilities):\n",
    "                index0=29\n",
    "            else:\n",
    "                index0=alphabet[word0[letter_ind]]\n",
    "                \n",
    "            if (word1[letter_ind] not in possibilities):\n",
    "                index1=29\n",
    "            else:\n",
    "                index1=alphabet[word1[letter_ind]]\n",
    "                \n",
    "            if (word2[letter_ind] not in possibilities):\n",
    "                index2=29\n",
    "            else:\n",
    "                index2=alphabet[word2[letter_ind]]\n",
    "                \n",
    "            Y[i,letter_ind*n_alphabet+index0]=1\n",
    "            X[i,letter_ind*n_alphabet+index1]=1\n",
    "            Y[i,3*n_alphabet+letter_ind*n_alphabet+index2]=1\n",
    "        \n",
    "    return torch.tensor(X, requires_grad=True).float(),torch.tensor(Y, requires_grad=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 90])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_maker(100)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\"> 3.2 Encoder/decoder to mimic FastText</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model takes a \"word\" (3 letters patch) as input and return a size 100 vector who is encoded to take into consideration the past and future patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedd(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        #A sequential container: Modules will be added to it in the order they are passed in the constructor. \n",
    "        self.encoder=nn.Sequential(\n",
    "            \n",
    "        nn.Linear(3*n_alphabet,70,bias=True),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(70,45,bias=True),\n",
    "        nn.Sigmoid())\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        \n",
    "        nn.Linear(45,110, bias=True),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(110,6*n_alphabet,bias=True),\n",
    "        )\n",
    "        \n",
    "        #The forward() method of Sequential accepts \n",
    "        #any input and forwards it to the first module it contains.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# PARAMS\n",
    "\n",
    "BATCH_SIZE=1000\n",
    "NUM_BACKWARDS=10000\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "LOAD_MODEL = True\n",
    "\n",
    "###############\n",
    "\n",
    "loss_list=[]\n",
    "Emb=embedd().to(device)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    Emb.load_state_dict(torch.load(\"Model.h5\"))\n",
    "else:\n",
    "    Emb.train()\n",
    "    optimizer = Adam(Emb.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "    for n in tqdm(range(NUM_BACKWARDS)):\n",
    "        \n",
    "        x,y_target=batch_maker(BATCH_SIZE)\n",
    "        x = x.to(device) #we put the model and the variable on the gpu\n",
    "        y=Emb(x)\n",
    "        y = y.to(device)\n",
    "        y_target = y_target.to(device) #because we use y_target after and we neet to put it on the gpu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_func=nn.CrossEntropyLoss()\n",
    "        loss1_target = (y_target[:,:n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss2_target = (y_target[:,n_alphabet:2*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss3_target = (y_target[:,2*n_alphabet:3*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        loss4_target = (y_target[:,3*n_alphabet:4*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss5_target = (y_target[:,4*n_alphabet:5*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss6_target = (y_target[:,5*n_alphabet:6*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        loss1=loss_func(y[:,:n_alphabet],loss1_target)\n",
    "        loss2=loss_func(y[:,n_alphabet:2*n_alphabet],loss2_target)\n",
    "        loss3=loss_func(y[:,2*n_alphabet:3*n_alphabet],loss3_target)\n",
    "        \n",
    "        loss4=loss_func(y[:,3*n_alphabet:4*n_alphabet],loss4_target)\n",
    "        loss5=loss_func(y[:,4*n_alphabet:5*n_alphabet],loss5_target)\n",
    "        loss6=loss_func(y[:,5*n_alphabet:6*n_alphabet],loss6_target)\n",
    "        \n",
    "        \n",
    "        loss=(loss1+loss2+loss3+loss4+loss5+loss6)/6\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item()) #All this to update the loss plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_list)\n",
    "        fig.savefig(\"loss plot embedd bigbatch.png\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        if NUM_BACKWARDS%1000==0:\n",
    "            torch.save(Emb.state_dict(),\"Model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_output(Model, word):\n",
    "\n",
    "    rev_alphabet={0:'_', 1:'a', 2:'b', 3:'c', 4:'d',5:'e',6:'f',7:'g',8:'h',9:'i',10:'j',11:'k',12:'l',\n",
    "                 13:'m',14:'n',15:'o',16:'p',17:'q',18:'r',19:'s',20:'t',21:'u',22:'v',23:'w',24:'x',\n",
    "                 25:'y',26:'z',27:'é',28:'è',29:'others'}\n",
    "\n",
    "    alphabet={\n",
    "            ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "            'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "            'è':28}\n",
    "\n",
    "    possibilities=[        \n",
    "            ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "            'y','z','é','è',]\n",
    "\n",
    "    inp=np.zeros((1,3*n_alphabet))\n",
    "    for letter_ind in range(3):\n",
    "        if word[letter_ind] in possibilities:\n",
    "            inp[0,letter_ind*n_alphabet+alphabet[word[letter_ind]]]=1\n",
    "        else:\n",
    "            inp[0,letter_ind*n_alphabet+29]=1\n",
    "\n",
    "    inp=torch.tensor(inp, requires_grad=True).float()\n",
    "    y=Model(inp.to(device))\n",
    "    y = y.cpu()\n",
    "\n",
    "    y00=np.argmax(y[:,:n_alphabet].detach().numpy())\n",
    "    y01=np.argmax(y[:,n_alphabet:2*n_alphabet].detach().numpy())\n",
    "    y02=np.argmax(y[:,2*n_alphabet:3*n_alphabet].detach().numpy())\n",
    "    y10=np.argmax(y[:,3*n_alphabet:4*n_alphabet].detach().numpy())\n",
    "    y11=np.argmax(y[:,4*n_alphabet:5*n_alphabet].detach().numpy())\n",
    "    y12=np.argmax(y[:,5*n_alphabet:6*n_alphabet].detach().numpy())\n",
    "\n",
    "    before=rev_alphabet[y00]+rev_alphabet[y01]+rev_alphabet[y02]\n",
    "    after=rev_alphabet[y10]+rev_alphabet[y11]+rev_alphabet[y12]\n",
    "\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('_ma', 'n__')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Emb = embedd()\n",
    "#Emb.load_state_dict(torch.load(\"C:/Users/feoni/OneDrive/Bureau/Polytechnique/ML/DeepLearning_Project/Model.h5\"))\n",
    "#Emb.eval()\n",
    "\n",
    "find_output(Emb, \"cro\")\n",
    "#Encoder avec plus de perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_encoded(df, line):\n",
    "    \n",
    "    cut=cutter(df[\"text\"][line],3)\n",
    "    length=cut.shape[0]\n",
    "    X=np.zeros((length,3*n_alphabet))\n",
    "    \n",
    "    alphabet={\n",
    "        ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "        'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "        'è':28,\n",
    "        #thus use mod 30 (as 29 is for any other letter (arabic...))\n",
    "        }\n",
    "        \n",
    "    possibilities=[        \n",
    "        ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "        'y','z','é','è',]\n",
    "\n",
    "    for word_index in range(length):\n",
    "\n",
    "        word=cut[word_index,1]\n",
    "        inp=np.zeros(3*n_alphabet)\n",
    "\n",
    "        for letter_ind in range(3):\n",
    "            \n",
    "            index=0\n",
    "\n",
    "            if (word[letter_ind] not in possibilities):\n",
    "                index0=29\n",
    "            else:\n",
    "                index0=alphabet[word[letter_ind]]\n",
    "                \n",
    "\n",
    "            X[word_index,letter_ind*n_alphabet+index0]=1\n",
    "\n",
    "        \n",
    "    return Emb.encoder(torch.tensor(X, requires_grad=True).float().to(device))#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8223, 0.2261, 0.8208, 0.8241, 0.5557, 0.2595, 0.5573, 0.5733, 0.0806,\n",
       "         0.0239, 0.6891, 0.2196, 0.2209, 0.5990, 0.8919, 0.9206, 0.2708, 0.3076,\n",
       "         0.4675, 0.2187, 0.8067, 0.0669, 0.8678, 0.6768, 0.0477, 0.4120, 0.4137,\n",
       "         0.6062, 0.2296, 0.4518, 0.1442, 0.8090, 0.9532, 0.3318, 0.8183, 0.2753,\n",
       "         0.6855, 0.3344, 0.6050, 0.9705, 0.9005, 0.4857, 0.9307, 0.3302, 0.0529],\n",
       "        [0.9955, 0.1860, 0.9436, 0.8721, 0.2193, 0.6894, 0.8869, 0.2775, 0.8995,\n",
       "         0.8266, 0.8841, 0.3540, 0.6382, 0.8770, 0.6117, 0.2996, 0.0775, 0.9558,\n",
       "         0.5516, 0.8851, 0.6858, 0.4825, 0.8610, 0.0313, 0.3631, 0.3780, 0.5814,\n",
       "         0.4109, 0.8921, 0.9181, 0.1364, 0.9728, 0.8243, 0.1617, 0.3826, 0.0589,\n",
       "         0.4889, 0.7155, 0.8793, 0.3931, 0.5750, 0.4299, 0.2341, 0.5651, 0.5942],\n",
       "        [0.4809, 0.9429, 0.5859, 0.7591, 0.4716, 0.6337, 0.4583, 0.5440, 0.7112,\n",
       "         0.8922, 0.5824, 0.4694, 0.8352, 0.0660, 0.2168, 0.0088, 0.3626, 0.4835,\n",
       "         0.2788, 0.4502, 0.7511, 0.7248, 0.1286, 0.4592, 0.7001, 0.6296, 0.3264,\n",
       "         0.7369, 0.2504, 0.2897, 0.8299, 0.3506, 0.4935, 0.0912, 0.8461, 0.1879,\n",
       "         0.3923, 0.8357, 0.9600, 0.0769, 0.4783, 0.3380, 0.7661, 0.9745, 0.7782],\n",
       "        [0.9873, 0.2331, 0.3788, 0.1151, 0.2498, 0.1232, 0.5050, 0.6209, 0.2413,\n",
       "         0.8431, 0.6340, 0.5360, 0.8635, 0.0548, 0.6271, 0.0152, 0.3528, 0.4025,\n",
       "         0.0090, 0.5703, 0.0203, 0.2172, 0.1768, 0.0359, 0.8073, 0.8692, 0.6401,\n",
       "         0.7610, 0.3078, 0.7508, 0.1674, 0.7330, 0.1808, 0.2681, 0.7831, 0.1619,\n",
       "         0.7058, 0.2165, 0.9635, 0.1259, 0.3816, 0.2900, 0.9918, 0.9721, 0.8156],\n",
       "        [0.6651, 0.7736, 0.8693, 0.4679, 0.5446, 0.0716, 0.2399, 0.9357, 0.5625,\n",
       "         0.3440, 0.8839, 0.3960, 0.0260, 0.8460, 0.6582, 0.1376, 0.6212, 0.0930,\n",
       "         0.6558, 0.3617, 0.4471, 0.1643, 0.7778, 0.8767, 0.4346, 0.1540, 0.4545,\n",
       "         0.1520, 0.0211, 0.4268, 0.7671, 0.7420, 0.7045, 0.1067, 0.4262, 0.7994,\n",
       "         0.4078, 0.2572, 0.0781, 0.6029, 0.7214, 0.3948, 0.7273, 0.9614, 0.0454],\n",
       "        [0.9553, 0.2384, 0.2270, 0.9233, 0.0879, 0.1085, 0.3643, 0.7713, 0.2459,\n",
       "         0.0332, 0.5705, 0.2858, 0.3989, 0.7774, 0.9749, 0.9414, 0.6396, 0.4382,\n",
       "         0.3313, 0.2727, 0.3571, 0.4710, 0.9566, 0.3833, 0.1393, 0.2659, 0.4420,\n",
       "         0.1620, 0.5651, 0.0942, 0.1119, 0.7731, 0.6728, 0.4743, 0.6374, 0.2752,\n",
       "         0.5827, 0.0991, 0.8156, 0.9173, 0.8022, 0.1134, 0.8924, 0.3751, 0.1123],\n",
       "        [0.8383, 0.8996, 0.9769, 0.6908, 0.6465, 0.5674, 0.7634, 0.9088, 0.9284,\n",
       "         0.3533, 0.9177, 0.2823, 0.0286, 0.9137, 0.6490, 0.7880, 0.5526, 0.8160,\n",
       "         0.8357, 0.7318, 0.8142, 0.5203, 0.8755, 0.7082, 0.4804, 0.4001, 0.7775,\n",
       "         0.5111, 0.4888, 0.3461, 0.7159, 0.3448, 0.8505, 0.0129, 0.3262, 0.0862,\n",
       "         0.3903, 0.4834, 0.4854, 0.2887, 0.8176, 0.3319, 0.0803, 0.8620, 0.2724],\n",
       "        [0.3938, 0.5153, 0.6444, 0.6405, 0.6439, 0.7971, 0.3264, 0.3619, 0.5468,\n",
       "         0.2691, 0.0853, 0.5695, 0.0412, 0.4846, 0.6697, 0.2818, 0.2827, 0.4190,\n",
       "         0.3362, 0.3297, 0.9447, 0.1137, 0.2899, 0.9121, 0.0836, 0.6107, 0.1021,\n",
       "         0.6037, 0.2528, 0.4792, 0.2921, 0.5142, 0.9273, 0.1971, 0.9316, 0.3941,\n",
       "         0.8589, 0.6760, 0.8994, 0.7944, 0.4997, 0.3727, 0.9500, 0.4576, 0.4920],\n",
       "        [0.9746, 0.5172, 0.9295, 0.7169, 0.4663, 0.5532, 0.5346, 0.7621, 0.7943,\n",
       "         0.6046, 0.7209, 0.4453, 0.2892, 0.8266, 0.4457, 0.2719, 0.2504, 0.7772,\n",
       "         0.3441, 0.7455, 0.6778, 0.2756, 0.7787, 0.1722, 0.7669, 0.2171, 0.6032,\n",
       "         0.5792, 0.6036, 0.8601, 0.6891, 0.8771, 0.6344, 0.0534, 0.4639, 0.0882,\n",
       "         0.2132, 0.4757, 0.6673, 0.4117, 0.2508, 0.4091, 0.3499, 0.7601, 0.4899],\n",
       "        [0.8780, 0.6023, 0.1726, 0.8479, 0.5578, 0.8071, 0.1094, 0.7779, 0.6126,\n",
       "         0.6701, 0.1075, 0.8857, 0.5234, 0.4294, 0.5774, 0.2064, 0.1253, 0.3500,\n",
       "         0.0917, 0.1760, 0.3447, 0.2385, 0.5097, 0.2672, 0.8593, 0.3431, 0.0360,\n",
       "         0.5230, 0.1368, 0.9391, 0.4274, 0.8402, 0.6064, 0.1217, 0.8204, 0.7073,\n",
       "         0.8460, 0.3754, 0.6361, 0.2449, 0.0659, 0.6827, 0.9148, 0.8474, 0.7971],\n",
       "        [0.9968, 0.6626, 0.7993, 0.7442, 0.1281, 0.2809, 0.3574, 0.8004, 0.6698,\n",
       "         0.8986, 0.9534, 0.5066, 0.2018, 0.8867, 0.7328, 0.1596, 0.2348, 0.6760,\n",
       "         0.6325, 0.4796, 0.3030, 0.3208, 0.5967, 0.0484, 0.6187, 0.3213, 0.3883,\n",
       "         0.1167, 0.4561, 0.9583, 0.5286, 0.9645, 0.6024, 0.1711, 0.3467, 0.6213,\n",
       "         0.5974, 0.4811, 0.8505, 0.5437, 0.3362, 0.2776, 0.6063, 0.8718, 0.7343],\n",
       "        [0.9631, 0.3420, 0.0516, 0.6527, 0.0927, 0.1815, 0.2439, 0.5894, 0.4309,\n",
       "         0.3031, 0.1643, 0.7879, 0.3730, 0.2529, 0.8563, 0.2738, 0.4776, 0.1789,\n",
       "         0.0466, 0.2283, 0.1320, 0.3238, 0.4159, 0.2629, 0.4010, 0.3085, 0.4695,\n",
       "         0.2176, 0.2379, 0.4869, 0.3738, 0.5160, 0.4139, 0.2898, 0.4060, 0.3344,\n",
       "         0.2624, 0.0291, 0.7466, 0.6437, 0.1183, 0.0753, 0.9523, 0.5885, 0.3533],\n",
       "        [0.3358, 0.7351, 0.7759, 0.7324, 0.1284, 0.0031, 0.0328, 0.9758, 0.2307,\n",
       "         0.0534, 0.8734, 0.1578, 0.3257, 0.8476, 0.8928, 0.1995, 0.7762, 0.3086,\n",
       "         0.2427, 0.4217, 0.0751, 0.0722, 0.9040, 0.5883, 0.0425, 0.2913, 0.7096,\n",
       "         0.0911, 0.0157, 0.1146, 0.3075, 0.5239, 0.4700, 0.3470, 0.7487, 0.7642,\n",
       "         0.0355, 0.1611, 0.0407, 0.9020, 0.8737, 0.1903, 0.9933, 0.9875, 0.0100],\n",
       "        [0.8539, 0.0691, 0.2345, 0.9489, 0.0825, 0.3544, 0.2499, 0.1318, 0.0313,\n",
       "         0.0312, 0.6045, 0.3448, 0.3438, 0.8684, 0.9624, 0.8033, 0.1809, 0.4687,\n",
       "         0.1512, 0.6242, 0.5534, 0.1516, 0.9465, 0.2980, 0.0202, 0.1017, 0.4676,\n",
       "         0.2658, 0.6236, 0.3412, 0.0590, 0.8978, 0.9356, 0.5349, 0.8574, 0.5165,\n",
       "         0.4861, 0.0810, 0.0787, 0.9755, 0.8613, 0.4024, 0.9949, 0.2812, 0.1135],\n",
       "        [0.9717, 0.6443, 0.9674, 0.6377, 0.1713, 0.2368, 0.7636, 0.7735, 0.8279,\n",
       "         0.6448, 0.9495, 0.3571, 0.8002, 0.9607, 0.3961, 0.6407, 0.0433, 0.7902,\n",
       "         0.6144, 0.7483, 0.7489, 0.4003, 0.5986, 0.1688, 0.8691, 0.4984, 0.9134,\n",
       "         0.2705, 0.6797, 0.6279, 0.5615, 0.8397, 0.9164, 0.0280, 0.6096, 0.1542,\n",
       "         0.2440, 0.6355, 0.5744, 0.6738, 0.5349, 0.1112, 0.2510, 0.6208, 0.1394],\n",
       "        [0.7989, 0.6104, 0.5066, 0.6691, 0.2622, 0.7896, 0.5827, 0.8543, 0.7185,\n",
       "         0.6893, 0.2582, 0.4670, 0.4761, 0.5057, 0.3398, 0.1293, 0.3662, 0.7392,\n",
       "         0.4488, 0.7596, 0.5160, 0.4641, 0.3716, 0.3360, 0.8112, 0.2216, 0.5956,\n",
       "         0.4017, 0.3884, 0.5294, 0.6540, 0.5365, 0.5082, 0.5066, 0.8552, 0.2301,\n",
       "         0.6140, 0.6514, 0.8099, 0.2771, 0.3193, 0.6389, 0.8893, 0.8639, 0.3697],\n",
       "        [0.7940, 0.8302, 0.9083, 0.4507, 0.2848, 0.2137, 0.5546, 0.9579, 0.7212,\n",
       "         0.5726, 0.8063, 0.4783, 0.4947, 0.7258, 0.1097, 0.1133, 0.3092, 0.9100,\n",
       "         0.7889, 0.7468, 0.1570, 0.1737, 0.3663, 0.1754, 0.6362, 0.6560, 0.8333,\n",
       "         0.4633, 0.3135, 0.7049, 0.6426, 0.2276, 0.5732, 0.6356, 0.9748, 0.4952,\n",
       "         0.4504, 0.5004, 0.6431, 0.3709, 0.8027, 0.6442, 0.9530, 0.9856, 0.0747],\n",
       "        [0.9514, 0.1318, 0.8616, 0.4874, 0.3859, 0.3024, 0.3562, 0.7560, 0.7031,\n",
       "         0.3787, 0.4401, 0.2739, 0.6652, 0.2715, 0.4273, 0.1793, 0.6627, 0.9419,\n",
       "         0.5815, 0.8483, 0.2470, 0.2798, 0.8742, 0.0562, 0.6775, 0.6204, 0.2868,\n",
       "         0.1495, 0.5440, 0.8491, 0.1017, 0.9124, 0.4376, 0.2718, 0.5903, 0.4630,\n",
       "         0.6444, 0.4261, 0.8214, 0.2289, 0.2885, 0.2473, 0.7118, 0.7657, 0.7346],\n",
       "        [0.8960, 0.2348, 0.5370, 0.1318, 0.0707, 0.0818, 0.4754, 0.6057, 0.4388,\n",
       "         0.5577, 0.1769, 0.6735, 0.3176, 0.3013, 0.8135, 0.1553, 0.6159, 0.5002,\n",
       "         0.0371, 0.2624, 0.0620, 0.2993, 0.6101, 0.0365, 0.6722, 0.7709, 0.3049,\n",
       "         0.2647, 0.3742, 0.7305, 0.3421, 0.5848, 0.3171, 0.2467, 0.4940, 0.3388,\n",
       "         0.4432, 0.2020, 0.9434, 0.2822, 0.0872, 0.0807, 0.5832, 0.8732, 0.5541]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoded(df, 2) #give the tensor of the complete embedded tweet (each 3 letter)\n",
    "\n",
    "#sentence_encoded(3).shape\n",
    "#30 for the alphabet size \n",
    "#the first argument is the tweet lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">4. NN coupled with RNN to find the rt </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feelings=5 #return of RNN part\n",
    "n_mainstream=5+3 #followers, likes, verified... data that didn't get treated (no need) (after normalisation) +time\n",
    "n_PCA_words=4 #after PCA on words\n",
    "n_PCA_hashtag=3 #after PCA on hashtags\n",
    "\n",
    "final_dim=n_feelings+n_mainstream+n_PCA_words+n_PCA_hashtag\n",
    "final_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">4.1 RNN on embedded words: Understanding relations between words to extract a general opinion/feeling from the tweet</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeelingsFinder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #RNN layer (30 for the input word vectors and 5 for the feeling vector of previous run)\n",
    "        self.recurrNN=nn.RNN(45, n_feelings)\n",
    "        #30 is the input vector and n_feelings the output vector\n",
    "        self.lin=nn.Linear(n_feelings,n_feelings)\n",
    "        #the rnn output 2 vectors : 1 output \"normal\" and one for the repetition\n",
    "        #the two args need to have the same size because the layer is expected to receive a same size input \n",
    "        \n",
    "    def forward(self,x): #x is a matrix of vectorised sentence\n",
    "            \n",
    "        #initialize first feeling vector \n",
    "        h=torch.zeros(1,n_feelings).to(device)\n",
    "        \n",
    "        #feed forward (x of shape (number of patches of 3, 100))\n",
    "        for i in range(x.shape[0]):\n",
    "            #we dont have to pay attention to the \"h\", because the thing we return is the \"out\" after it passed through the linear layer\n",
    "            out, _ =self.recurrNN(x[i:i+1,:],h)\n",
    "            out=self.lin(out)\n",
    "            h=out\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "ff = FeelingsFinder().to(device)\n",
    "ff\n",
    "\n",
    "dummy_input = torch.rand((40, 45)).to(device)\n",
    "dummy_input\n",
    "\n",
    "print(ff(dummy_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">4.2 NN to find out rt: takes as input the output of RNN & all the treated dimensions above (2 PCAs, normalised mainstream dimensions...)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTFinder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.NN=nn.Sequential(\n",
    "        \n",
    "        nn.Linear(final_dim,10,bias=True),\n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        nn.Linear(10,4,bias=True),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        nn.Linear(4,1, bias=True),\n",
    "        nn.Sigmoid())\n",
    "        \n",
    "        self.feelings_finder = FeelingsFinder()\n",
    "        \n",
    "    def forward(self, x, y): #x is concatenatioon of all dimensions and y same as x in forward of FeelingsFinder\n",
    "        \n",
    "        feelings = self.feelings_finder(y)\n",
    "        \n",
    "        nn_input = torch.cat((x.T,feelings.T)).T\n",
    "        \n",
    "        return self.NN(nn_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6100]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.rand((40, 45)).to(device)\n",
    "\n",
    "RT= RTFinder().to(device)\n",
    "\n",
    "dummy_input2 = torch.rand((1, 15)).to(device)\n",
    "dummy_input3 = torch.rand((1, 15)).to(device)\n",
    "\n",
    "#print(torch.cat((dummy_input2.T, dummy_input3.T)).T.shape)\n",
    "print(RT(dummy_input2, dummy_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_train(line):\n",
    "    concat_data=np.zeros((1, 15))\n",
    "    concat_data[0,0]=df[\"favorites_count\"][line]\n",
    "    concat_data[0,1]=df[\"followers_count\"][line]\n",
    "    concat_data[0,2]=df[\"friends_count\"][line]\n",
    "    concat_data[0,3]=df[\"statuses_count\"][line]\n",
    "    concat_data[0,4]=df[\"verified\"][line]\n",
    "    concat_data[0,5]=df[\"month\"][line]\n",
    "    concat_data[0,6]=df[\"day\"][line]\n",
    "    concat_data[0,7]=df[\"moment\"][line]\n",
    "    \n",
    "    #try to define the most important words so no need to redefine them later\n",
    "    \n",
    "    concat_data[0,8:12]=PCA_TRAIN_TEXT[0][line,:]\n",
    "\n",
    "    concat_data[0,12:15]=PCA_TRAIN_HASHTAGS[0][line,:]\n",
    "    \n",
    "    return torch.tensor(concat_data).float(), sentence_encoded(df, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.4428,  0.5961,  0.4815,  0.0000,  0.1818,  0.4333,  0.8881,\n",
       "          -0.2179, -0.1484,  0.1105,  0.1800,  0.1854,  0.0083,  0.0022]]),\n",
       " tensor([[0.6704, 0.1157, 0.7614, 0.8849, 0.2707, 0.3005, 0.4581, 0.2693, 0.0393,\n",
       "          0.0146, 0.7338, 0.1586, 0.1123, 0.6990, 0.9009, 0.8215, 0.4171, 0.4458,\n",
       "          0.3714, 0.5788, 0.7958, 0.0716, 0.9002, 0.7023, 0.0117, 0.1235, 0.3699,\n",
       "          0.2989, 0.2616, 0.1331, 0.1938, 0.6419, 0.9308, 0.6614, 0.9384, 0.5302,\n",
       "          0.6481, 0.2489, 0.2672, 0.9890, 0.9232, 0.6852, 0.9856, 0.2599, 0.0209],\n",
       "         [0.9413, 0.1977, 0.8826, 0.7486, 0.3641, 0.7051, 0.7489, 0.9003, 0.7746,\n",
       "          0.7915, 0.7720, 0.6632, 0.6763, 0.9084, 0.5067, 0.2657, 0.1467, 0.8891,\n",
       "          0.6270, 0.8342, 0.4608, 0.5373, 0.7615, 0.1582, 0.8636, 0.4720, 0.4310,\n",
       "          0.1257, 0.7957, 0.6810, 0.5286, 0.9319, 0.1723, 0.2109, 0.5779, 0.1875,\n",
       "          0.6070, 0.7455, 0.8629, 0.4135, 0.1539, 0.7513, 0.0590, 0.7664, 0.1997],\n",
       "         [0.3798, 0.7382, 0.7557, 0.6001, 0.4855, 0.6766, 0.4974, 0.8592, 0.8773,\n",
       "          0.8064, 0.2755, 0.6295, 0.6323, 0.4891, 0.4797, 0.0446, 0.4046, 0.8630,\n",
       "          0.5156, 0.7029, 0.4683, 0.5673, 0.1559, 0.5493, 0.8424, 0.8121, 0.3871,\n",
       "          0.5308, 0.7433, 0.3312, 0.7899, 0.2789, 0.5448, 0.1866, 0.9843, 0.3311,\n",
       "          0.7195, 0.6837, 0.9722, 0.1483, 0.3902, 0.5809, 0.8376, 0.9684, 0.6133],\n",
       "         [0.9810, 0.6808, 0.5850, 0.8240, 0.3444, 0.3113, 0.1017, 0.9909, 0.3702,\n",
       "          0.7252, 0.9114, 0.7031, 0.7308, 0.8714, 0.3921, 0.1691, 0.0802, 0.6429,\n",
       "          0.8059, 0.2873, 0.1068, 0.0663, 0.9800, 0.1759, 0.8463, 0.4854, 0.2386,\n",
       "          0.2560, 0.0516, 0.9202, 0.1867, 0.9907, 0.2637, 0.2650, 0.7644, 0.6520,\n",
       "          0.6338, 0.1728, 0.5544, 0.5990, 0.3036, 0.8332, 0.7321, 0.9484, 0.3703],\n",
       "         [0.9971, 0.5321, 0.3581, 0.3458, 0.0093, 0.1076, 0.6853, 0.9236, 0.4014,\n",
       "          0.6030, 0.6321, 0.9391, 0.1930, 0.9100, 0.9648, 0.9092, 0.2381, 0.2389,\n",
       "          0.0752, 0.1056, 0.0290, 0.1123, 0.8900, 0.0136, 0.7034, 0.4689, 0.3553,\n",
       "          0.0779, 0.1991, 0.8449, 0.3611, 0.9371, 0.3391, 0.2470, 0.2990, 0.1562,\n",
       "          0.4515, 0.0233, 0.9163, 0.8251, 0.0960, 0.0276, 0.2430, 0.5666, 0.2203],\n",
       "         [0.2337, 0.8907, 0.1919, 0.6931, 0.4254, 0.0090, 0.0203, 0.8032, 0.3583,\n",
       "          0.0134, 0.0680, 0.6476, 0.1438, 0.8285, 0.8015, 0.2407, 0.4985, 0.1640,\n",
       "          0.5371, 0.1304, 0.4569, 0.1392, 0.1413, 0.9304, 0.2027, 0.3141, 0.8550,\n",
       "          0.6220, 0.1291, 0.3425, 0.9393, 0.0350, 0.7865, 0.2048, 0.9874, 0.8471,\n",
       "          0.0309, 0.0855, 0.0485, 0.9831, 0.3200, 0.0131, 0.9897, 0.9872, 0.0567],\n",
       "         [0.9770, 0.1004, 0.1671, 0.9980, 0.3562, 0.4819, 0.1918, 0.7794, 0.0682,\n",
       "          0.0108, 0.9530, 0.6408, 0.5265, 0.4061, 0.9618, 0.9716, 0.0350, 0.3058,\n",
       "          0.4478, 0.7216, 0.7003, 0.1158, 0.9917, 0.6089, 0.0405, 0.0487, 0.1385,\n",
       "          0.2307, 0.2484, 0.3387, 0.0262, 0.9946, 0.9100, 0.4569, 0.8405, 0.4341,\n",
       "          0.9024, 0.0410, 0.1835, 0.9924, 0.8626, 0.5690, 0.9980, 0.2905, 0.0315],\n",
       "         [0.9828, 0.7764, 0.9557, 0.6014, 0.3852, 0.6516, 0.8425, 0.9725, 0.8962,\n",
       "          0.8294, 0.8979, 0.7915, 0.1446, 0.7539, 0.3923, 0.8289, 0.5070, 0.9255,\n",
       "          0.9436, 0.9042, 0.2453, 0.3526, 0.6346, 0.2159, 0.4591, 0.3776, 0.6233,\n",
       "          0.1238, 0.3226, 0.7667, 0.7456, 0.8989, 0.1691, 0.2702, 0.7971, 0.1786,\n",
       "          0.6687, 0.7800, 0.8719, 0.2612, 0.3105, 0.6722, 0.1158, 0.7926, 0.1223],\n",
       "         [0.7704, 0.2253, 0.2240, 0.7812, 0.7252, 0.7663, 0.1064, 0.5174, 0.6412,\n",
       "          0.4144, 0.0637, 0.8922, 0.3391, 0.0899, 0.5809, 0.2667, 0.1744, 0.4520,\n",
       "          0.2878, 0.2055, 0.2832, 0.0974, 0.8565, 0.4798, 0.4296, 0.5228, 0.0364,\n",
       "          0.5161, 0.1028, 0.9101, 0.0665, 0.9557, 0.6331, 0.1204, 0.6978, 0.4386,\n",
       "          0.7687, 0.0752, 0.5965, 0.2572, 0.0661, 0.4374, 0.8826, 0.6477, 0.7093],\n",
       "         [0.8245, 0.0972, 0.8498, 0.1620, 0.0839, 0.3170, 0.8390, 0.4307, 0.3509,\n",
       "          0.6815, 0.3873, 0.4076, 0.5967, 0.4232, 0.1449, 0.1958, 0.0659, 0.8918,\n",
       "          0.2755, 0.7841, 0.2231, 0.2101, 0.4106, 0.0499, 0.5323, 0.8338, 0.5880,\n",
       "          0.1804, 0.4554, 0.8220, 0.1873, 0.9092, 0.5495, 0.5336, 0.6498, 0.1109,\n",
       "          0.3084, 0.3815, 0.8541, 0.1517, 0.0792, 0.1710, 0.4136, 0.5759, 0.4632],\n",
       "         [0.8866, 0.9677, 0.4132, 0.8478, 0.0995, 0.5310, 0.4998, 0.6349, 0.9648,\n",
       "          0.7840, 0.3133, 0.7893, 0.3901, 0.2646, 0.4759, 0.1712, 0.5694, 0.8390,\n",
       "          0.1699, 0.7335, 0.1343, 0.5690, 0.5449, 0.2131, 0.3484, 0.2187, 0.3093,\n",
       "          0.4312, 0.1981, 0.3114, 0.7666, 0.3205, 0.2586, 0.1679, 0.9628, 0.1893,\n",
       "          0.2350, 0.4017, 0.8101, 0.1394, 0.5803, 0.5133, 0.9389, 0.9592, 0.3899],\n",
       "         [0.8373, 0.8778, 0.9360, 0.5115, 0.5063, 0.2990, 0.7037, 0.8538, 0.8545,\n",
       "          0.7951, 0.9046, 0.1483, 0.4100, 0.9016, 0.1275, 0.0291, 0.3459, 0.9356,\n",
       "          0.6878, 0.6467, 0.2878, 0.4190, 0.8213, 0.2041, 0.5423, 0.5469, 0.8852,\n",
       "          0.6462, 0.4748, 0.6155, 0.4979, 0.1571, 0.6566, 0.3121, 0.9406, 0.5221,\n",
       "          0.1070, 0.9200, 0.1522, 0.1031, 0.9341, 0.7417, 0.9296, 0.9907, 0.0997],\n",
       "         [0.9573, 0.4046, 0.8452, 0.4926, 0.2141, 0.5864, 0.2853, 0.8118, 0.9132,\n",
       "          0.8295, 0.8104, 0.5764, 0.2488, 0.9390, 0.6792, 0.3228, 0.2274, 0.4875,\n",
       "          0.5488, 0.2246, 0.8161, 0.3189, 0.7833, 0.2233, 0.5731, 0.2948, 0.4520,\n",
       "          0.4852, 0.3553, 0.8652, 0.2143, 0.9217, 0.8009, 0.2209, 0.2270, 0.3595,\n",
       "          0.5805, 0.3202, 0.9162, 0.6894, 0.5095, 0.4499, 0.1846, 0.3096, 0.7034],\n",
       "         [0.5682, 0.8259, 0.2667, 0.9880, 0.1776, 0.5909, 0.1370, 0.4818, 0.7746,\n",
       "          0.3256, 0.4003, 0.2954, 0.9638, 0.6300, 0.5191, 0.3699, 0.0648, 0.7876,\n",
       "          0.8197, 0.4009, 0.9609, 0.9043, 0.2612, 0.4792, 0.6920, 0.8719, 0.5352,\n",
       "          0.1555, 0.7895, 0.3584, 0.3586, 0.7983, 0.7174, 0.0187, 0.1513, 0.4004,\n",
       "          0.2165, 0.0957, 0.8318, 0.4584, 0.2968, 0.1501, 0.1207, 0.4361, 0.6800],\n",
       "         [0.9492, 0.8435, 0.1999, 0.4427, 0.1180, 0.0789, 0.1874, 0.8394, 0.4139,\n",
       "          0.1186, 0.0557, 0.9386, 0.8390, 0.1000, 0.9013, 0.6189, 0.7250, 0.2065,\n",
       "          0.0484, 0.2208, 0.1334, 0.2150, 0.2076, 0.3214, 0.7613, 0.8129, 0.2974,\n",
       "          0.6788, 0.0831, 0.7510, 0.5792, 0.6990, 0.3716, 0.1582, 0.7862, 0.6015,\n",
       "          0.7282, 0.0403, 0.9433, 0.6755, 0.2985, 0.0504, 0.9722, 0.9554, 0.5810]],\n",
       "        device='cuda:0', grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_train(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d85bd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353969"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['TweetID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/3539690 [00:00<?, ?it/s]/tmp/ipykernel_23904/2973378327.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss=loss_func(rt, torch.tensor(rt_target).float())\n",
      "/tmp/ipykernel_23904/2973378327.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss+=loss_func(rt, torch.tensor(rt_target).float())\n",
      "  0%|                                                                                    | 747/3539690 [00:09<11:20:17, 86.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 1492/3539690 [00:49<11:13:38, 87.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 2249/3539690 [01:19<11:55:06, 82.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.105264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 2989/3539690 [01:30<11:39:12, 84.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 3744/3539690 [02:09<12:42:44, 77.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 4493/3539690 [02:39<10:14:47, 95.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.106355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 5245/3539690 [02:49<11:15:35, 87.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 5990/3539690 [03:29<10:59:07, 89.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.107071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 6741/3539690 [03:59<10:24:53, 94.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 7492/3539690 [04:19<12:00:37, 81.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 8247/3539690 [04:49<11:36:11, 84.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 8996/3539690 [05:19<13:11:05, 74.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.100421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 9744/3539690 [05:30<11:11:36, 87.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.106881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 10496/3539690 [06:09<13:01:56, 75.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 11244/3539690 [06:39<11:09:54, 87.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                  | 11998/3539690 [07:09<9:51:24, 99.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 12745/3539690 [07:29<10:18:45, 95.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 13492/3539690 [07:59<12:07:11, 80.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 14244/3539690 [08:11<10:36:43, 92.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 14999/3539690 [08:49<10:39:21, 91.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 15746/3539690 [09:19<10:28:32, 93.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 16494/3539690 [09:30<12:04:45, 81.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 17243/3539690 [10:09<9:11:21, 106.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 17999/3539690 [10:39<10:31:09, 93.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 18744/3539690 [10:51<10:51:31, 90.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 19494/3539690 [11:29<12:34:46, 77.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.101709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 20247/3539690 [11:59<11:43:14, 83.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.105097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 20997/3539690 [12:29<11:20:52, 86.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 21741/3539690 [12:49<10:56:48, 89.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.105183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 22498/3539690 [13:19<10:39:19, 91.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 23243/3539690 [13:49<11:59:43, 81.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.099928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 23994/3539690 [14:00<11:32:18, 84.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 24749/3539690 [14:39<12:19:57, 79.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.106579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 25494/3539690 [15:09<11:33:05, 84.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.105947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 26246/3539690 [15:21<12:20:37, 79.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.101234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 26992/3539690 [15:59<19:29:29, 50.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.105582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 27747/3539690 [16:29<12:32:29, 77.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 28496/3539690 [16:41<10:49:10, 90.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 29241/3539690 [17:20<10:35:01, 92.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 29994/3539690 [17:50<11:33:15, 84.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 30748/3539690 [18:20<11:21:54, 85.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.104009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 31497/3539690 [18:30<12:16:33, 79.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.103648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 32249/3539690 [19:10<11:38:45, 83.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.102657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 32993/3539690 [19:23<12:13:04, 79.73it/s]"
     ]
    }
   ],
   "source": [
    "#Training loop (train for each line)\n",
    "\n",
    "\n",
    "UPDATE_EVERY = 750\n",
    "BIG_RANGE=len(df['TweetID'])\n",
    "\n",
    "RT=RTFinder().to(device)\n",
    "progressbar = tqdm(range(10*BIG_RANGE))\n",
    "loss=None\n",
    "loss_list=[]\n",
    "\n",
    "#optimizer = Adam(RT.parameters(),lr=0.001)\n",
    "\n",
    "for line in progressbar:\n",
    "    line=line%BIG_RANGE\n",
    "    #if loss is None:\n",
    "        #optimizer.zero_grad()\n",
    "    x0, x1 = data_to_train(line)\n",
    "    x0 = x0.to(device)\n",
    "    x1 = x1.to(device)\n",
    "    rt=RT(x0, x1)\n",
    "    rt_target=df[\"retweets_count\"][line]\n",
    "    rt_target = torch.tensor(rt_target).to(device)\n",
    "    loss_func=nn.MSELoss()\n",
    "    \n",
    "    if loss is None:\n",
    "        loss=loss_func(rt, torch.tensor(rt_target).float())\n",
    "    else:\n",
    "        loss+=loss_func(rt, torch.tensor(rt_target).float())\n",
    "    \n",
    "    if line%UPDATE_EVERY==UPDATE_EVERY-1:\n",
    "        loss/=UPDATE_EVERY\n",
    "        torch.save(Emb.state_dict(),\"Final_Network.h5\")\n",
    "        \n",
    "        loss_list.append(loss.item()) #All this to update the loss plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_list)\n",
    "        fig.savefig(\"loss plot.png\")\n",
    "        plt.close(fig)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Loss: {loss.item(): .6f}\")\n",
    "        progressbar.set_description(f\"Loss: {loss.item(): .6f}\")\n",
    "        loss=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">5. Apply everything onto test dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">5.1 Prepare test data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=Normaliser(df, df_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_test(line):\n",
    "    concat_data=np.zeros((1,15))\n",
    "    concat_data[0,0]=df_test[\"favorites_count\"][line]\n",
    "    concat_data[0,1]=df_test[\"followers_count\"][line]\n",
    "    concat_data[0,2]=df_test[\"friends_count\"][line]\n",
    "    concat_data[0,3]=df_test[\"statuses_count\"][line]\n",
    "    concat_data[0,4]=df_test[\"verified\"][line]\n",
    "    concat_data[0,5]=df_test[\"month\"][line]\n",
    "    concat_data[0,6]=df_test[\"day\"][line]\n",
    "    concat_data[0,7]=df_test[\"moment\"][line]\n",
    "    \n",
    "    PCA_text=PCA_test(array_train_text,array_test_text,4)[line,:]\n",
    "    concat_data[0,8:12]=PCA_text\n",
    "    \n",
    "    PCA_hashtag=PCA_test(array_train_hashtags,array_test_hashtags,3)[line,:]\n",
    "    concat_data[0,12:15]=PCA_hashtag\n",
    "    \n",
    "    return torch.tensor(concat_data).float(), sentence_encoded(df_test, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_to_test(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">5.2 Predict on the test data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function : Mean Absolute Error (MAE) ->\n",
    "The MAE metric is calculated by dividing the sum of absolute differences between the predicted\n",
    "number of retweets (pi) and the observed number of retweets (ai) by the number of observations\n",
    "(N), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, predictions):\n",
    "    y_true, predictions = np.array(y_true), np.array(predictions)\n",
    "    return np.mean(np.abs(y_true - predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval=mae(Y,predictions)\n",
    "print(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "58644109350f8e12cb67e1c1e77a7e72950bcd98a8aaf135bac033a9e112be78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
