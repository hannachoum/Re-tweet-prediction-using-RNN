{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d73323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c13ff",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">1. Preprocessing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06532983",
   "metadata": {},
   "source": [
    "##  <span style=\"color:indigo\"> 1.1 Dataset observation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3260f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecture du fichier .csv par la libraire Pandas\n",
    "data=pd.read_csv('train.csv')\n",
    "df1=data.copy()\n",
    "df_test1=pd.read_csv('evaluation.csv')\n",
    "\n",
    "#We need to separate X and Y\n",
    "#X=data.loc[:,data.columns!=\"retweets_count\"]\n",
    "#Y=data['retweets_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f01832",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">1.2 Features explanations </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03fc63",
   "metadata": {},
   "source": [
    "df.quantile(0.99) #We check if there is a lot ludicrous values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc394984",
   "metadata": {},
   "source": [
    "#If our dataset contains empty values, we put the mean value of the collum\n",
    "if df.isnull().sum().sum()!=0: \n",
    "    imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "else: \n",
    "    print(\"No missing_values in this dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee09ca7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(28,24))\n",
    "sns.heatmap(data = corr_matrix,cmap='BrBG', annot=True, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555418d0",
   "metadata": {},
   "source": [
    " ------> We observe that the only strong correlation that we can see at this time is the one between [favorite_count] \n",
    "and [retweet_count] , so we can already say that to predict the number of retweet, the number of favorite count is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16545ab",
   "metadata": {},
   "source": [
    "#checking for missing values in output\n",
    "for i in range(df.shape[0]):\n",
    "    if df['retweets_count'][i]==[]:\n",
    "        print(df['retweets_count'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cb0c3",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">1.3 Normalisations </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4a204",
   "metadata": {},
   "source": [
    "On doit normaliser les données mais chaque colonne doit être normalisé différemment. \n",
    "- \"favorites_count\" -> moins de 1% de valeurs abhérantes (au top)\n",
    "- \"followers_count\", -> Exponential \n",
    "- \"statutes_count\",\n",
    "- \"friends_count\" -> moins de 1% de valeurs abhérantes (au top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f5be6",
   "metadata": {},
   "source": [
    "Normalization.quantile(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6882438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_into_df(df):\n",
    "    result=pd.DataFrame(df,columns=[\"month\", \"day\", \"moment\"])\n",
    "    timestamps=np.array(df[\"timestamp\"])\n",
    "    \n",
    "    for i in range(timestamps.shape[0]):\n",
    "        date=dt.fromtimestamp(timestamps[i]/1000)\n",
    "        result[\"month\"][i]=date.month\n",
    "        result[\"day\"][i]=date.day\n",
    "        result[\"moment\"][i]=date.hour+date.minute/60\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af76d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x):\n",
    "     return np.log(1+x)\n",
    "\n",
    "def Normaliser(df, df_test):\n",
    "    \n",
    "    column_names=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"]\n",
    "    \n",
    "    #Create DFs and apply log to them\n",
    "    \n",
    "    Norm_train=pd.DataFrame(df,columns=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"])\n",
    "    Norm_train['favorites_count'] = Norm_train['favorites_count'].transform(myfunc)\n",
    "    Norm_train['followers_count'] = Norm_train['followers_count'].transform(myfunc)\n",
    "    Norm_train['friends_count'] = Norm_train['friends_count'].transform(myfunc)\n",
    "    Norm_train['statuses_count'] = Norm_train['statuses_count'].transform(myfunc)\n",
    "    \n",
    "    Norm_rt=pd.DataFrame(df,columns=[\"retweets_count\"])\n",
    "    Norm_rt['retweets_count'] = Norm_rt['retweets_count'].transform(myfunc)\n",
    "    \n",
    "    Norm_test=pd.DataFrame(df_test,columns=[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\"])\n",
    "    Norm_test['favorites_count'] = Norm_test['favorites_count'].transform(myfunc)\n",
    "    Norm_test['followers_count'] = Norm_test['followers_count'].transform(myfunc)\n",
    "    Norm_test['friends_count'] = Norm_test['friends_count'].transform(myfunc)\n",
    "    Norm_test['statuses_count'] = Norm_test['statuses_count'].transform(myfunc)\n",
    "    \n",
    "    #Also add time\n",
    "    \n",
    "    Time_train=get_time_into_df(df)\n",
    "    Time_test=get_time_into_df(df_test)\n",
    "    \n",
    "    #Now rescale with min-max\n",
    "    \n",
    "    scaler_train=MinMaxScaler()\n",
    "    scaler_train.fit(Norm_train)\n",
    "    norm_train=scaler_train.transform(Norm_train)\n",
    "    norm_test=scaler_train.transform(Norm_test)\n",
    "    \n",
    "    scaler_rt=MinMaxScaler()\n",
    "    scaler_rt.fit(Norm_rt)\n",
    "    norm_rt=scaler_rt.transform(Norm_rt)\n",
    "    \n",
    "    scaler_time=MinMaxScaler()\n",
    "    scaler_time.fit(Time_train)\n",
    "    norm_time_train=scaler_time.transform(Time_train)\n",
    "    norm_time_test=scaler_time.transform(Time_test)\n",
    "    \n",
    "    #Now put the rescaled results into the original DFs\n",
    "    \n",
    "    Norm_train=pd.DataFrame(norm_train,columns=column_names)\n",
    "    Norm_test=pd.DataFrame(norm_test,columns=column_names)\n",
    "    Norm_rt=pd.DataFrame(norm_rt,columns=[\"retweets_count\"])\n",
    "    \n",
    "    NormT_train=pd.DataFrame(norm_time_train,columns=[\"month\", \"day\", \"moment\"])\n",
    "    NormT_test=pd.DataFrame(norm_time_test,columns=[\"month\", \"day\", \"moment\"])\n",
    "    \n",
    "    df['favorites_count']=Norm_train['favorites_count']\n",
    "    df['followers_count']=Norm_train['followers_count'] \n",
    "    df['statuses_count']=Norm_train['statuses_count']\n",
    "    df['friends_count']=Norm_train['friends_count']\n",
    "    df['retweets_count']=Norm_rt['retweets_count']\n",
    "    \n",
    "    df_test['favorites_count']=Norm_test['favorites_count']\n",
    "    df_test['followers_count']=Norm_test['followers_count'] \n",
    "    df_test['statuses_count']=Norm_test['statuses_count']\n",
    "    df_test['friends_count']=Norm_test['friends_count']\n",
    "    \n",
    "    df[\"month\"]=NormT_train[\"month\"]\n",
    "    df[\"day\"]=NormT_train[\"day\"]\n",
    "    df[\"moment\"]=NormT_train[\"moment\"]\n",
    "    \n",
    "    df_test[\"month\"]=NormT_test[\"month\"]\n",
    "    df_test[\"day\"]=NormT_test[\"day\"]\n",
    "    df_test[\"moment\"]=NormT_test[\"moment\"]\n",
    "\n",
    "    return df, df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbc0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test= Normaliser(df1,df_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa752937",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">2. PCA Text and # treatment </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4d4cb",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:indigo\">2.1 Preprocessing of hashtags and texts </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0c214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for hashtags\n",
    "\n",
    "def most_important_hashtags(df, criteria):\n",
    "    DICO2={} #DICO2 is for hashtags\n",
    "    I=df.shape[0]\n",
    "    for i in range(I):\n",
    "        sentence=df[\"hashtags\"][i][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in DICO2:\n",
    "                DICO2[extracted_word]+=1\n",
    "            else:\n",
    "                DICO2[extracted_word]=1\n",
    "    most_occurr_DICO2={}\n",
    "    for word in DICO2:\n",
    "        if DICO2[word]>criteria :#and word!=''\n",
    "            most_occurr_DICO2[word]=0\n",
    "    \n",
    "    important_hashtags=list(most_occurr_DICO2.keys())\n",
    "    \n",
    "    measurer2={}\n",
    "    for i in range(len(important_hashtags)):\n",
    "        measurer2[important_hashtags[i]]=i\n",
    "    measurer2\n",
    "    return important_hashtags, measurer2\n",
    "\n",
    "def make_array_training_hashtags(df, criteria):\n",
    "    possibilities, measurer=most_important_hashtags(df, criteria)\n",
    "    result=np.zeros((df.shape[0], len(possibilities)))\n",
    "    for line in range(df.shape[0]):\n",
    "        sentence=df[\"hashtags\"][line][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in possibilities:\n",
    "                result[line,measurer[extracted_word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "def make_array_test_hashtags(df_test, criteria):\n",
    "    possibilities, measurer=most_important_hashtags(df, criteria)\n",
    "    result=np.zeros((df_test.shape[0], len(possibilities)))\n",
    "    for line in range(df_test.shape[0]):\n",
    "        sentence=df_test[\"hashtags\"][line][1:-1].replace(' ', '').split(',')\n",
    "        for word in sentence:\n",
    "            extracted_word=word[1:-1]\n",
    "            if extracted_word in possibilities:\n",
    "                result[line,measurer[extracted_word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "array_train_hashtags=make_array_training_hashtags(df,10)\n",
    "array_test_hashtags=make_array_test_hashtags(df_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612ac371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for texts\n",
    "\n",
    "def most_important_words(df, criteria):\n",
    "    DICO={}\n",
    "    I=df.shape[0]\n",
    "    for i in range(I):\n",
    "        sentence=df[\"text\"][i].split()\n",
    "        for word in sentence:\n",
    "            if word in DICO:\n",
    "                DICO[word]+=1\n",
    "            else:\n",
    "                DICO[word]=1\n",
    "            \n",
    "    most_occurr_DICO={}\n",
    "    for word in DICO:\n",
    "        if DICO[word]>criteria:\n",
    "            most_occurr_DICO[word]=0\n",
    "    \n",
    "    important_words=list(most_occurr_DICO.keys())\n",
    "    measurer={}\n",
    "    for i in range(len(important_words)):\n",
    "        measurer[important_words[i]]=i\n",
    "    measurer\n",
    "    \n",
    "    return important_words, measurer\n",
    "\n",
    "def make_array_training_words(df, criteria):\n",
    "    possibilities, measurer=most_important_words(df, criteria)\n",
    "    result=np.zeros((df.shape[0], len(possibilities)))\n",
    "    for line in range(df.shape[0]):\n",
    "        sentence=df[\"text\"][line].split()\n",
    "        for word in sentence:\n",
    "            if word in possibilities:\n",
    "                result[line,measurer[word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "def make_array_test_words(df_test, criteria):\n",
    "    possibilities, measurer=most_important_words(df, criteria)\n",
    "    result=np.zeros((df_test.shape[0], len(possibilities)))\n",
    "    for line in range(df_test.shape[0]):\n",
    "        sentence=df_test[\"text\"][line].split()\n",
    "        for word in sentence:\n",
    "            if word in possibilities:\n",
    "                result[line,measurer[word]]=1\n",
    "                \n",
    "    return result\n",
    "\n",
    "array_train_text=make_array_training_words(df,5000)\n",
    "array_test_text=make_array_test_words(df_test, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224661d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "## <span style=\"color:indigo\">2.2 PCA on words and hashtags: Understanding the main components of bags of words </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdada19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_training(array, n_comp):\n",
    "    n=array.shape[1]\n",
    "    for col in range(n):\n",
    "        mean=np.mean(array[:,col])\n",
    "        array[:,col]-=mean\n",
    "            \n",
    "    C=np.matmul(array.T,array)\n",
    "    C/=array.shape[1]\n",
    "    \n",
    "    Lambda, Q=np.linalg.eigh(C)\n",
    "\n",
    "    Qk=np.zeros((n,n_comp))\n",
    "    for col in range(n_comp):\n",
    "        Qk[:,col]=Q[:,n-col-1]\n",
    "    return np.matmul(array,Qk), Qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdd7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_test(training_array, test_array, n_comp):\n",
    "    return np.matmul(test_array,PCA_training(training_array,n_comp)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5e43e",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">3. Preparing RNN on word sequences: Understanding relations between words </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9396b7",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:indigo\"> 3.1 Pre-treatment for word embedding </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e31ed4",
   "metadata": {},
   "source": [
    "We will use the FastText method. We divide each word into groups of 3 letters (spaces are the \"empty letter\"). We keep the order in the text and make an embedding also using the adjacent words. For instance:\n",
    "\"macron demission\"==> -ma,mac,acr,...,on-,n-d,-de,dem,... (- is the empty letter)\n",
    "\n",
    "The fast text method will guess the adjacent 3 letters to each given patch of 3 letters. Hence for \"mac\" we want a high probability of \"---\" (3 times empty letter) and \"ron\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b2651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\"  \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d221d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns an array with the words in the sentence cut (n=3 that is in pieces of 3)\n",
    "\n",
    "def cutter(sent, n):\n",
    "    sentence=sent.split()\n",
    "    length=len(sentence)+2-n\n",
    "    for word in sentence:\n",
    "        length+=len(word)\n",
    "    result=[]\n",
    "    padding=''\n",
    "    for i in range(n):\n",
    "        padding+=' '\n",
    "    sent=' '+sent+' '\n",
    "    padd_sent=padding+sent+padding\n",
    "    \n",
    "    for i in range(length):\n",
    "        middle=sent[i:i+n]\n",
    "        before=padd_sent[i:i+n]\n",
    "        after=padd_sent[2*n+i:3*n+i]\n",
    "        result.append([before, middle, after])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669bec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphabet=30\n",
    "\n",
    "def batch_maker(size):\n",
    "    X=np.zeros((size,3*n_alphabet))\n",
    "    Y=np.zeros((size,6*n_alphabet))\n",
    "    for i in range(size):\n",
    "        \n",
    "        line=random.randrange(0,350000)\n",
    "        cut=cutter(df[\"text\"][line],3)\n",
    "        \n",
    "        alphabet={\n",
    "        ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "        'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "        'è':28,\n",
    "        #thus use mod 30 (as 29 is for any other letter (arabic...))\n",
    "        }\n",
    "        \n",
    "        possibilities=[        \n",
    "        ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "        'y','z','é','è',]\n",
    "        \n",
    "        length=cut.shape[0]\n",
    "        word_index=random.randrange(0,length)\n",
    "        \n",
    "        word0=cut[word_index, 0]\n",
    "        word1=cut[word_index, 1]\n",
    "        word2=cut[word_index, 2]\n",
    "        for letter_ind in range(3):\n",
    "            index0=0\n",
    "            index1=0\n",
    "            index2=0\n",
    "            if (word0[letter_ind] not in possibilities):\n",
    "                index0=29\n",
    "            else:\n",
    "                index0=alphabet[word0[letter_ind]]\n",
    "                \n",
    "            if (word1[letter_ind] not in possibilities):\n",
    "                index1=29\n",
    "            else:\n",
    "                index1=alphabet[word1[letter_ind]]\n",
    "                \n",
    "            if (word2[letter_ind] not in possibilities):\n",
    "                index2=29\n",
    "            else:\n",
    "                index2=alphabet[word2[letter_ind]]\n",
    "                \n",
    "            Y[i,letter_ind*n_alphabet+index0]=1\n",
    "            X[i,letter_ind*n_alphabet+index1]=1\n",
    "            Y[i,3*n_alphabet+letter_ind*n_alphabet+index2]=1\n",
    "        \n",
    "    return torch.tensor(X, requires_grad=True).float(),torch.tensor(Y, requires_grad=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b49c033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 90])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_maker(100)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99eaa7",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\"> 3.2 Encoder/decoder to mimic FastText</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212deded",
   "metadata": {},
   "source": [
    "This model takes a \"word\" (3 letters patch) as input and return a size 100 vector who is encoded to take into consideration the past and future patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b92c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedd(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        #A sequential container: Modules will be added to it in the order they are passed in the constructor. \n",
    "        self.encoder=nn.Sequential(\n",
    "            \n",
    "        nn.Linear(3*n_alphabet,70,bias=True),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(70,45,bias=True),\n",
    "        nn.Sigmoid())\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        \n",
    "        nn.Linear(45,110, bias=True),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(110,6*n_alphabet,bias=True),\n",
    "        )\n",
    "        \n",
    "        #The forward() method of Sequential accepts \n",
    "        #any input and forwards it to the first module it contains.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb78c0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# PARAMS\n",
    "\n",
    "BATCH_SIZE=1000\n",
    "NUM_BACKWARDS=10000\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "LOAD_MODEL = True\n",
    "\n",
    "###############\n",
    "\n",
    "loss_list=[]\n",
    "Emb=embedd().to(device)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    Emb.load_state_dict(torch.load(\"Model.h5\"))\n",
    "else:\n",
    "    Emb.train()\n",
    "    optimizer = Adam(Emb.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "    for n in tqdm(range(NUM_BACKWARDS)):\n",
    "        \n",
    "        x,y_target=batch_maker(BATCH_SIZE)\n",
    "        x = x.to(device) #we put the model and the variable on the gpu\n",
    "        y=Emb(x)\n",
    "        y = y.to(device)\n",
    "        y_target = y_target.to(device) #because we use y_target after and we neet to put it on the gpu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_func=nn.CrossEntropyLoss()\n",
    "        loss1_target = (y_target[:,:n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss2_target = (y_target[:,n_alphabet:2*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss3_target = (y_target[:,2*n_alphabet:3*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        loss4_target = (y_target[:,3*n_alphabet:4*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss5_target = (y_target[:,4*n_alphabet:5*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "        loss6_target = (y_target[:,5*n_alphabet:6*n_alphabet] == 1).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        loss1=loss_func(y[:,:n_alphabet],loss1_target)\n",
    "        loss2=loss_func(y[:,n_alphabet:2*n_alphabet],loss2_target)\n",
    "        loss3=loss_func(y[:,2*n_alphabet:3*n_alphabet],loss3_target)\n",
    "        \n",
    "        loss4=loss_func(y[:,3*n_alphabet:4*n_alphabet],loss4_target)\n",
    "        loss5=loss_func(y[:,4*n_alphabet:5*n_alphabet],loss5_target)\n",
    "        loss6=loss_func(y[:,5*n_alphabet:6*n_alphabet],loss6_target)\n",
    "        \n",
    "        \n",
    "        loss=(loss1+loss2+loss3+loss4+loss5+loss6)/6\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item()) #All this to update the loss plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_list)\n",
    "        fig.savefig(\"loss plot embedd bigbatch.png\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        if NUM_BACKWARDS%1000==0:\n",
    "            torch.save(Emb.state_dict(),\"Model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c8db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_output(Model, word):\n",
    "\n",
    "    rev_alphabet={0:'_', 1:'a', 2:'b', 3:'c', 4:'d',5:'e',6:'f',7:'g',8:'h',9:'i',10:'j',11:'k',12:'l',\n",
    "                 13:'m',14:'n',15:'o',16:'p',17:'q',18:'r',19:'s',20:'t',21:'u',22:'v',23:'w',24:'x',\n",
    "                 25:'y',26:'z',27:'é',28:'è',29:'others'}\n",
    "\n",
    "    alphabet={\n",
    "            ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "            'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "            'è':28}\n",
    "\n",
    "    possibilities=[        \n",
    "            ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "            'y','z','é','è',]\n",
    "\n",
    "    inp=np.zeros((1,3*n_alphabet))\n",
    "    for letter_ind in range(3):\n",
    "        if word[letter_ind] in possibilities:\n",
    "            inp[0,letter_ind*n_alphabet+alphabet[word[letter_ind]]]=1\n",
    "        else:\n",
    "            inp[0,letter_ind*n_alphabet+29]=1\n",
    "\n",
    "    inp=torch.tensor(inp, requires_grad=True).float()\n",
    "    y=Model(inp.to(device))\n",
    "    y = y.cpu()\n",
    "\n",
    "    y00=np.argmax(y[:,:n_alphabet].detach().numpy())\n",
    "    y01=np.argmax(y[:,n_alphabet:2*n_alphabet].detach().numpy())\n",
    "    y02=np.argmax(y[:,2*n_alphabet:3*n_alphabet].detach().numpy())\n",
    "    y10=np.argmax(y[:,3*n_alphabet:4*n_alphabet].detach().numpy())\n",
    "    y11=np.argmax(y[:,4*n_alphabet:5*n_alphabet].detach().numpy())\n",
    "    y12=np.argmax(y[:,5*n_alphabet:6*n_alphabet].detach().numpy())\n",
    "\n",
    "    before=rev_alphabet[y00]+rev_alphabet[y01]+rev_alphabet[y02]\n",
    "    after=rev_alphabet[y10]+rev_alphabet[y11]+rev_alphabet[y12]\n",
    "\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccddd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('_ma', 'n__')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Emb = embedd()\n",
    "#Emb.load_state_dict(torch.load(\"C:/Users/feoni/OneDrive/Bureau/Polytechnique/ML/DeepLearning_Project/Model.h5\"))\n",
    "#Emb.eval()\n",
    "\n",
    "find_output(Emb, \"cro\")\n",
    "#Encoder avec plus de perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c57efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_encoded(df, line):\n",
    "    \n",
    "    cut=cutter(df[\"text\"][line],3)\n",
    "    length=cut.shape[0]\n",
    "    X=np.zeros((length,3*n_alphabet))\n",
    "    \n",
    "    alphabet={\n",
    "        ' ':0,'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,\n",
    "        'o':15,'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'é':27,\n",
    "        'è':28,\n",
    "        #thus use mod 30 (as 29 is for any other letter (arabic...))\n",
    "        }\n",
    "        \n",
    "    possibilities=[        \n",
    "        ' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x',\n",
    "        'y','z','é','è',]\n",
    "\n",
    "    for word_index in range(length):\n",
    "\n",
    "        word=cut[word_index,1]\n",
    "        inp=np.zeros(3*n_alphabet)\n",
    "\n",
    "        for letter_ind in range(3):\n",
    "            \n",
    "            index=0\n",
    "\n",
    "            if (word[letter_ind] not in possibilities):\n",
    "                index0=29\n",
    "            else:\n",
    "                index0=alphabet[word[letter_ind]]\n",
    "                \n",
    "\n",
    "            X[word_index,letter_ind*n_alphabet+index0]=1\n",
    "\n",
    "        \n",
    "    return Emb.encoder(torch.tensor(X, requires_grad=True).float().to(device))#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e774a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8223, 0.2261, 0.8208, 0.8241, 0.5557, 0.2595, 0.5573, 0.5733, 0.0806,\n",
       "         0.0239, 0.6891, 0.2196, 0.2209, 0.5990, 0.8919, 0.9206, 0.2708, 0.3076,\n",
       "         0.4675, 0.2187, 0.8067, 0.0669, 0.8678, 0.6768, 0.0477, 0.4120, 0.4137,\n",
       "         0.6062, 0.2296, 0.4518, 0.1442, 0.8090, 0.9532, 0.3318, 0.8183, 0.2753,\n",
       "         0.6855, 0.3344, 0.6050, 0.9705, 0.9005, 0.4857, 0.9307, 0.3302, 0.0529],\n",
       "        [0.9955, 0.1860, 0.9436, 0.8721, 0.2193, 0.6894, 0.8869, 0.2775, 0.8995,\n",
       "         0.8266, 0.8841, 0.3540, 0.6382, 0.8770, 0.6117, 0.2996, 0.0775, 0.9558,\n",
       "         0.5516, 0.8851, 0.6858, 0.4825, 0.8610, 0.0313, 0.3631, 0.3780, 0.5814,\n",
       "         0.4109, 0.8921, 0.9181, 0.1364, 0.9728, 0.8243, 0.1617, 0.3826, 0.0589,\n",
       "         0.4889, 0.7155, 0.8793, 0.3931, 0.5750, 0.4299, 0.2341, 0.5651, 0.5942],\n",
       "        [0.4809, 0.9429, 0.5859, 0.7591, 0.4716, 0.6337, 0.4583, 0.5440, 0.7112,\n",
       "         0.8922, 0.5824, 0.4694, 0.8352, 0.0660, 0.2168, 0.0088, 0.3626, 0.4835,\n",
       "         0.2788, 0.4502, 0.7511, 0.7248, 0.1286, 0.4592, 0.7001, 0.6296, 0.3264,\n",
       "         0.7369, 0.2504, 0.2897, 0.8299, 0.3506, 0.4935, 0.0912, 0.8461, 0.1879,\n",
       "         0.3923, 0.8357, 0.9600, 0.0769, 0.4783, 0.3380, 0.7661, 0.9745, 0.7782],\n",
       "        [0.9873, 0.2331, 0.3788, 0.1151, 0.2498, 0.1232, 0.5050, 0.6209, 0.2413,\n",
       "         0.8431, 0.6340, 0.5360, 0.8635, 0.0548, 0.6271, 0.0152, 0.3528, 0.4025,\n",
       "         0.0090, 0.5703, 0.0203, 0.2172, 0.1768, 0.0359, 0.8073, 0.8692, 0.6401,\n",
       "         0.7610, 0.3078, 0.7508, 0.1674, 0.7330, 0.1808, 0.2681, 0.7831, 0.1619,\n",
       "         0.7058, 0.2165, 0.9635, 0.1259, 0.3816, 0.2900, 0.9918, 0.9721, 0.8156],\n",
       "        [0.6651, 0.7736, 0.8693, 0.4679, 0.5446, 0.0716, 0.2399, 0.9357, 0.5625,\n",
       "         0.3440, 0.8839, 0.3960, 0.0260, 0.8460, 0.6582, 0.1376, 0.6212, 0.0930,\n",
       "         0.6558, 0.3617, 0.4471, 0.1643, 0.7778, 0.8767, 0.4346, 0.1540, 0.4545,\n",
       "         0.1520, 0.0211, 0.4268, 0.7671, 0.7420, 0.7045, 0.1067, 0.4262, 0.7994,\n",
       "         0.4078, 0.2572, 0.0781, 0.6029, 0.7214, 0.3948, 0.7273, 0.9614, 0.0454],\n",
       "        [0.9553, 0.2384, 0.2270, 0.9233, 0.0879, 0.1085, 0.3643, 0.7713, 0.2459,\n",
       "         0.0332, 0.5705, 0.2858, 0.3989, 0.7774, 0.9749, 0.9414, 0.6396, 0.4382,\n",
       "         0.3313, 0.2727, 0.3571, 0.4710, 0.9566, 0.3833, 0.1393, 0.2659, 0.4420,\n",
       "         0.1620, 0.5651, 0.0942, 0.1119, 0.7731, 0.6728, 0.4743, 0.6374, 0.2752,\n",
       "         0.5827, 0.0991, 0.8156, 0.9173, 0.8022, 0.1134, 0.8924, 0.3751, 0.1123],\n",
       "        [0.8383, 0.8996, 0.9769, 0.6908, 0.6465, 0.5674, 0.7634, 0.9088, 0.9284,\n",
       "         0.3533, 0.9177, 0.2823, 0.0286, 0.9137, 0.6490, 0.7880, 0.5526, 0.8160,\n",
       "         0.8357, 0.7318, 0.8142, 0.5203, 0.8755, 0.7082, 0.4804, 0.4001, 0.7775,\n",
       "         0.5111, 0.4888, 0.3461, 0.7159, 0.3448, 0.8505, 0.0129, 0.3262, 0.0862,\n",
       "         0.3903, 0.4834, 0.4854, 0.2887, 0.8176, 0.3319, 0.0803, 0.8620, 0.2724],\n",
       "        [0.3938, 0.5153, 0.6444, 0.6405, 0.6439, 0.7971, 0.3264, 0.3619, 0.5468,\n",
       "         0.2691, 0.0853, 0.5695, 0.0412, 0.4846, 0.6697, 0.2818, 0.2827, 0.4190,\n",
       "         0.3362, 0.3297, 0.9447, 0.1137, 0.2899, 0.9121, 0.0836, 0.6107, 0.1021,\n",
       "         0.6037, 0.2528, 0.4792, 0.2921, 0.5142, 0.9273, 0.1971, 0.9316, 0.3941,\n",
       "         0.8589, 0.6760, 0.8994, 0.7944, 0.4997, 0.3727, 0.9500, 0.4576, 0.4920],\n",
       "        [0.9746, 0.5172, 0.9295, 0.7169, 0.4663, 0.5532, 0.5346, 0.7621, 0.7943,\n",
       "         0.6046, 0.7209, 0.4453, 0.2892, 0.8266, 0.4457, 0.2719, 0.2504, 0.7772,\n",
       "         0.3441, 0.7455, 0.6778, 0.2756, 0.7787, 0.1722, 0.7669, 0.2171, 0.6032,\n",
       "         0.5792, 0.6036, 0.8601, 0.6891, 0.8771, 0.6344, 0.0534, 0.4639, 0.0882,\n",
       "         0.2132, 0.4757, 0.6673, 0.4117, 0.2508, 0.4091, 0.3499, 0.7601, 0.4899],\n",
       "        [0.8780, 0.6023, 0.1726, 0.8479, 0.5578, 0.8071, 0.1094, 0.7779, 0.6126,\n",
       "         0.6701, 0.1075, 0.8857, 0.5234, 0.4294, 0.5774, 0.2064, 0.1253, 0.3500,\n",
       "         0.0917, 0.1760, 0.3447, 0.2385, 0.5097, 0.2672, 0.8593, 0.3431, 0.0360,\n",
       "         0.5230, 0.1368, 0.9391, 0.4274, 0.8402, 0.6064, 0.1217, 0.8204, 0.7073,\n",
       "         0.8460, 0.3754, 0.6361, 0.2449, 0.0659, 0.6827, 0.9148, 0.8474, 0.7971],\n",
       "        [0.9968, 0.6626, 0.7993, 0.7442, 0.1281, 0.2809, 0.3574, 0.8004, 0.6698,\n",
       "         0.8986, 0.9534, 0.5066, 0.2018, 0.8867, 0.7328, 0.1596, 0.2348, 0.6760,\n",
       "         0.6325, 0.4796, 0.3030, 0.3208, 0.5967, 0.0484, 0.6187, 0.3213, 0.3883,\n",
       "         0.1167, 0.4561, 0.9583, 0.5286, 0.9645, 0.6024, 0.1711, 0.3467, 0.6213,\n",
       "         0.5974, 0.4811, 0.8505, 0.5437, 0.3362, 0.2776, 0.6063, 0.8718, 0.7343],\n",
       "        [0.9631, 0.3420, 0.0516, 0.6527, 0.0927, 0.1815, 0.2439, 0.5894, 0.4309,\n",
       "         0.3031, 0.1643, 0.7879, 0.3730, 0.2529, 0.8563, 0.2738, 0.4776, 0.1789,\n",
       "         0.0466, 0.2283, 0.1320, 0.3238, 0.4159, 0.2629, 0.4010, 0.3085, 0.4695,\n",
       "         0.2176, 0.2379, 0.4869, 0.3738, 0.5160, 0.4139, 0.2898, 0.4060, 0.3344,\n",
       "         0.2624, 0.0291, 0.7466, 0.6437, 0.1183, 0.0753, 0.9523, 0.5885, 0.3533],\n",
       "        [0.3358, 0.7351, 0.7759, 0.7324, 0.1284, 0.0031, 0.0328, 0.9758, 0.2307,\n",
       "         0.0534, 0.8734, 0.1578, 0.3257, 0.8476, 0.8928, 0.1995, 0.7762, 0.3086,\n",
       "         0.2427, 0.4217, 0.0751, 0.0722, 0.9040, 0.5883, 0.0425, 0.2913, 0.7096,\n",
       "         0.0911, 0.0157, 0.1146, 0.3075, 0.5239, 0.4700, 0.3470, 0.7487, 0.7642,\n",
       "         0.0355, 0.1611, 0.0407, 0.9020, 0.8737, 0.1903, 0.9933, 0.9875, 0.0100],\n",
       "        [0.8539, 0.0691, 0.2345, 0.9489, 0.0825, 0.3544, 0.2499, 0.1318, 0.0313,\n",
       "         0.0312, 0.6045, 0.3448, 0.3438, 0.8684, 0.9624, 0.8033, 0.1809, 0.4687,\n",
       "         0.1512, 0.6242, 0.5534, 0.1516, 0.9465, 0.2980, 0.0202, 0.1017, 0.4676,\n",
       "         0.2658, 0.6236, 0.3412, 0.0590, 0.8978, 0.9356, 0.5349, 0.8574, 0.5165,\n",
       "         0.4861, 0.0810, 0.0787, 0.9755, 0.8613, 0.4024, 0.9949, 0.2812, 0.1135],\n",
       "        [0.9717, 0.6443, 0.9674, 0.6377, 0.1713, 0.2368, 0.7636, 0.7735, 0.8279,\n",
       "         0.6448, 0.9495, 0.3571, 0.8002, 0.9607, 0.3961, 0.6407, 0.0433, 0.7902,\n",
       "         0.6144, 0.7483, 0.7489, 0.4003, 0.5986, 0.1688, 0.8691, 0.4984, 0.9134,\n",
       "         0.2705, 0.6797, 0.6279, 0.5615, 0.8397, 0.9164, 0.0280, 0.6096, 0.1542,\n",
       "         0.2440, 0.6355, 0.5744, 0.6738, 0.5349, 0.1112, 0.2510, 0.6208, 0.1394],\n",
       "        [0.7989, 0.6104, 0.5066, 0.6691, 0.2622, 0.7896, 0.5827, 0.8543, 0.7185,\n",
       "         0.6893, 0.2582, 0.4670, 0.4761, 0.5057, 0.3398, 0.1293, 0.3662, 0.7392,\n",
       "         0.4488, 0.7596, 0.5160, 0.4641, 0.3716, 0.3360, 0.8112, 0.2216, 0.5956,\n",
       "         0.4017, 0.3884, 0.5294, 0.6540, 0.5365, 0.5082, 0.5066, 0.8552, 0.2301,\n",
       "         0.6140, 0.6514, 0.8099, 0.2771, 0.3193, 0.6389, 0.8893, 0.8639, 0.3697],\n",
       "        [0.7940, 0.8302, 0.9083, 0.4507, 0.2848, 0.2137, 0.5546, 0.9579, 0.7212,\n",
       "         0.5726, 0.8063, 0.4783, 0.4947, 0.7258, 0.1097, 0.1133, 0.3092, 0.9100,\n",
       "         0.7889, 0.7468, 0.1570, 0.1737, 0.3663, 0.1754, 0.6362, 0.6560, 0.8333,\n",
       "         0.4633, 0.3135, 0.7049, 0.6426, 0.2276, 0.5732, 0.6356, 0.9748, 0.4952,\n",
       "         0.4504, 0.5004, 0.6431, 0.3709, 0.8027, 0.6442, 0.9530, 0.9856, 0.0747],\n",
       "        [0.9514, 0.1318, 0.8616, 0.4874, 0.3859, 0.3024, 0.3562, 0.7560, 0.7031,\n",
       "         0.3787, 0.4401, 0.2739, 0.6652, 0.2715, 0.4273, 0.1793, 0.6627, 0.9419,\n",
       "         0.5815, 0.8483, 0.2470, 0.2798, 0.8742, 0.0562, 0.6775, 0.6204, 0.2868,\n",
       "         0.1495, 0.5440, 0.8491, 0.1017, 0.9124, 0.4376, 0.2718, 0.5903, 0.4630,\n",
       "         0.6444, 0.4261, 0.8214, 0.2289, 0.2885, 0.2473, 0.7118, 0.7657, 0.7346],\n",
       "        [0.8960, 0.2348, 0.5370, 0.1318, 0.0707, 0.0818, 0.4754, 0.6057, 0.4388,\n",
       "         0.5577, 0.1769, 0.6735, 0.3176, 0.3013, 0.8135, 0.1553, 0.6159, 0.5002,\n",
       "         0.0371, 0.2624, 0.0620, 0.2993, 0.6101, 0.0365, 0.6722, 0.7709, 0.3049,\n",
       "         0.2647, 0.3742, 0.7305, 0.3421, 0.5848, 0.3171, 0.2467, 0.4940, 0.3388,\n",
       "         0.4432, 0.2020, 0.9434, 0.2822, 0.0872, 0.0807, 0.5832, 0.8732, 0.5541]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoded(df, 2) #give the tensor of the complete embedded tweet (each 3 letter)\n",
    "\n",
    "#sentence_encoded(3).shape\n",
    "#30 for the alphabet size \n",
    "#the first argument is the tweet lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557b530",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">4. NN coupled with RNN to find the rt </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ef932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feelings=5 #return of RNN part\n",
    "n_mainstream=5+3 #followers, likes, verified... data that didn't get treated (no need) (after normalisation) +time\n",
    "n_PCA_words=4 #after PCA on words\n",
    "n_PCA_hashtag=3 #after PCA on hashtags\n",
    "\n",
    "final_dim=n_feelings+n_mainstream+n_PCA_words+n_PCA_hashtag\n",
    "final_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1c976",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">4.1 RNN on embedded words: Understanding relations between words to extract a general opinion/feeling from the tweet</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6999e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeelingsFinder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #RNN layer (30 for the input word vectors and 5 for the feeling vector of previous run)\n",
    "        self.recurrNN=nn.RNN(45, n_feelings)\n",
    "        #30 is the input vector and n_feelings the output vector\n",
    "        self.lin=nn.Linear(n_feelings,n_feelings)\n",
    "        #the rnn output 2 vectors : 1 output \"normal\" and one for the repetition\n",
    "        #the two args need to have the same size because the layer is expected to receive a same size input \n",
    "        \n",
    "    def forward(self,x): #x is a matrix of vectorised sentence\n",
    "            \n",
    "        #initialize first feeling vector \n",
    "        h=torch.zeros(1,n_feelings).to(device)\n",
    "        \n",
    "        #feed forward (x of shape (number of patches of 3, 100))\n",
    "        for i in range(x.shape[0]):\n",
    "            #we dont have to pay attention to the \"h\", because the thing we return is the \"out\" after it passed through the linear layer\n",
    "            out, _ =self.recurrNN(x[i:i+1,:],h)\n",
    "            out=self.lin(out)\n",
    "            h=out\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69794723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "ff = FeelingsFinder().to(device)\n",
    "ff\n",
    "\n",
    "dummy_input = torch.rand((40, 45)).to(device)\n",
    "dummy_input\n",
    "\n",
    "print(ff(dummy_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be5e67",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">4.2 NN to find out rt: takes as input the output of RNN & all the treated dimensions above (2 PCAs, normalised mainstream dimensions...)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc576147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTFinder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.NN=nn.Sequential(\n",
    "        \n",
    "        nn.Linear(final_dim,10,bias=True),\n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        nn.Linear(10,4,bias=True),\n",
    "        nn.ReLU(True),\n",
    "        \n",
    "        nn.Linear(4,1, bias=True),\n",
    "        nn.Sigmoid())\n",
    "        \n",
    "        self.feelings_finder = FeelingsFinder()\n",
    "        \n",
    "    def forward(self, x, y): #x is concatenatioon of all dimensions and y same as x in forward of FeelingsFinder\n",
    "        \n",
    "        feelings = self.feelings_finder(y)\n",
    "        \n",
    "        nn_input = torch.cat((x.T,feelings.T)).T\n",
    "        \n",
    "        print(nn_input.shape)\n",
    "        \n",
    "        return self.NN(nn_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2eed56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.4881]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.rand((40, 45)).to(device)\n",
    "\n",
    "RT= RTFinder().to(device)\n",
    "\n",
    "dummy_input2 = torch.rand((1, 15)).to(device)\n",
    "dummy_input3 = torch.rand((1, 15)).to(device)\n",
    "\n",
    "#print(torch.cat((dummy_input2.T, dummy_input3.T)).T.shape)\n",
    "print(RT(dummy_input2, dummy_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09a0fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_train(line):\n",
    "    concat_data=np.zeros((1, 15))\n",
    "    concat_data[0,0]=df[\"favorites_count\"][line]\n",
    "    concat_data[0,1]=df[\"followers_count\"][line]\n",
    "    concat_data[0,2]=df[\"friends_count\"][line]\n",
    "    concat_data[0,3]=df[\"statuses_count\"][line]\n",
    "    concat_data[0,4]=df[\"verified\"][line]\n",
    "    concat_data[0,5]=df[\"month\"][line]\n",
    "    concat_data[0,6]=df[\"day\"][line]\n",
    "    concat_data[0,7]=df[\"moment\"][line]\n",
    "    \n",
    "    #try to define the most important words so no need to redefine them later\n",
    "    \n",
    "    PCA_text=PCA_training(array_train_text,4)[0][line,:]\n",
    "    concat_data[0,8:12]=PCA_text\n",
    "    \n",
    "    PCA_hashtag=PCA_training(array_train_hashtags,3)[0][line,:]\n",
    "    concat_data[0,12:15]=PCA_hashtag\n",
    "    \n",
    "    return torch.tensor(concat_data).float(), sentence_encoded(df, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee192c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.4480,  0.5473,  0.7705,  0.0000,  0.1818,  0.4667,  0.4844,\n",
       "          -0.2490, -0.1701,  0.1340,  0.3202,  0.1854,  0.0083,  0.0022]]),\n",
       " tensor([[0.3300, 0.2475, 0.3455,  ..., 0.9748, 0.4048, 0.0377],\n",
       "         [0.9005, 0.3743, 0.8699,  ..., 0.0995, 0.5244, 0.4619],\n",
       "         [0.6293, 0.9763, 0.5330,  ..., 0.7590, 0.9689, 0.5178],\n",
       "         ...,\n",
       "         [0.8044, 0.9331, 0.9682,  ..., 0.0343, 0.9669, 0.3269],\n",
       "         [0.8758, 0.8481, 0.2319,  ..., 0.9890, 0.9502, 0.9085],\n",
       "         [0.9978, 0.5686, 0.5444,  ..., 0.5229, 0.7803, 0.3395]],\n",
       "        device='cuda:0', grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acaef2c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/35000 [00:00<?, ?it/s]/tmp/ipykernel_22238/4194703111.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss=loss_func(rt, torch.tensor(rt_target).float())\n",
      "/home/julien/dev/perso/Tweet_Prediction/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|                                                                                                                           | 1/35000 [00:09<95:21:40,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22238/4194703111.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss+=loss_func(rt, torch.tensor(rt_target).float())\n",
      "  0%|                                                                                                                           | 2/35000 [00:20<99:07:47, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 3/35000 [00:30<99:46:41, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 4/35000 [00:40<99:49:26, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 5/35000 [00:50<98:14:31, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 6/35000 [01:00<97:51:43, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 7/35000 [01:10<98:26:26, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 8/35000 [01:20<97:23:20, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 9/35000 [01:30<96:00:01,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 10/35000 [01:40<96:36:55,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 11/35000 [01:50<96:26:22,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 12/35000 [02:00<96:38:50,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 13/35000 [02:10<96:02:55,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 14/35000 [02:19<95:59:05,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 15/35000 [02:29<96:16:09,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 16/35000 [02:39<95:55:23,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 17/35000 [02:49<96:04:48,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 18/35000 [02:59<96:32:49,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 19/35000 [03:09<96:12:51,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 20/35000 [03:19<96:18:53,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 21/35000 [03:29<96:36:22,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 22/35000 [03:39<96:54:43,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 23/35000 [03:49<96:26:10,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 24/35000 [03:58<95:56:14,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 25/35000 [04:08<95:28:00,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 26/35000 [04:18<96:18:25,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 27/35000 [04:28<96:53:37,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 27/35000 [04:38<100:15:59, 10.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m x0, x1 \u001b[39m=\u001b[39m data_to_train(line)\n\u001b[1;32m     14\u001b[0m x0 \u001b[39m=\u001b[39m x0\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m x1 \u001b[39m=\u001b[39m x1\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mdata_to_train\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     14\u001b[0m PCA_text\u001b[39m=\u001b[39mPCA_training(array_train_text,\u001b[39m4\u001b[39m)[\u001b[39m0\u001b[39m][line,:]\n\u001b[1;32m     15\u001b[0m concat_data[\u001b[39m0\u001b[39m,\u001b[39m8\u001b[39m:\u001b[39m12\u001b[39m]\u001b[39m=\u001b[39mPCA_text\n\u001b[0;32m---> 17\u001b[0m PCA_hashtag\u001b[39m=\u001b[39mPCA_training(array_train_hashtags,\u001b[39m3\u001b[39;49m)[\u001b[39m0\u001b[39m][line,:]\n\u001b[1;32m     18\u001b[0m concat_data[\u001b[39m0\u001b[39m,\u001b[39m12\u001b[39m:\u001b[39m15\u001b[39m]\u001b[39m=\u001b[39mPCA_hashtag\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(concat_data)\u001b[39m.\u001b[39mfloat(), sentence_encoded(df, line)\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mPCA_training\u001b[0;34m(array, n_comp)\u001b[0m\n\u001b[1;32m      4\u001b[0m     mean\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mmean(array[:,col])\n\u001b[1;32m      5\u001b[0m     array[:,col]\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mmean\n\u001b[0;32m----> 7\u001b[0m C\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(array\u001b[39m.\u001b[39;49mT,array)\n\u001b[1;32m      8\u001b[0m C\u001b[39m/\u001b[39m\u001b[39m=\u001b[39marray\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m Lambda, Q\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(C)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training loop (train for each line)\n",
    "\n",
    "\n",
    "UPDATE_EVERY = 100\n",
    "\n",
    "RT=RTFinder().to(device)\n",
    "progressbar = tqdm(range(35000))\n",
    "loss=None\n",
    "loss_list=[]\n",
    "\n",
    "optimizer = Adam(RT.parameters(),lr=0.001)\n",
    "\n",
    "for line in progressbar:\n",
    "    if loss is None:\n",
    "        optimizer.zero_grad()\n",
    "    x0, x1 = data_to_train(line)\n",
    "    x0 = x0.to(device)\n",
    "    x1 = x1.to(device)\n",
    "    rt=RT(x0, x1)\n",
    "    rt_target=df[\"retweets_count\"][line]\n",
    "    rt_target = torch.tensor(rt_target).to(device)\n",
    "    loss_func=nn.MSELoss()\n",
    "    \n",
    "    if loss is None:\n",
    "        loss=loss_func(rt, torch.tensor(rt_target).float())\n",
    "    else:\n",
    "        loss+=loss_func(rt, torch.tensor(rt_target).float())\n",
    "    \n",
    "    if line%UPDATE_EVERY==UPDATE_EVERY-1:\n",
    "        line/=UPDATE_EVERY\n",
    "        torch.save(Emb.state_dict(),\"Final_Network.h5\")\n",
    "        \n",
    "        loss_list.append(loss.item()) #All this to update the loss plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_list)\n",
    "        fig.savefig(\"loss plot.png\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss: {loss.item(): .6f}\")\n",
    "        #progressbar.set_description(f\"Loss: {loss.item(): .6f}\")\n",
    "        loss=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79616f1b",
   "metadata": {},
   "source": [
    "#  <span style=\"color:darkred\">5. Apply everything onto test dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33963a77",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">5.1 Prepare test data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700562be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e672ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=Normaliser(df, df_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_test(line):\n",
    "    concat_data=np.zeros((1,15))\n",
    "    concat_data[0,0]=df_test[\"favorites_count\"][line]\n",
    "    concat_data[0,1]=df_test[\"followers_count\"][line]\n",
    "    concat_data[0,2]=df_test[\"friends_count\"][line]\n",
    "    concat_data[0,3]=df_test[\"statuses_count\"][line]\n",
    "    concat_data[0,4]=df_test[\"verified\"][line]\n",
    "    concat_data[0,5]=df_test[\"month\"][line]\n",
    "    concat_data[0,6]=df_test[\"day\"][line]\n",
    "    concat_data[0,7]=df_test[\"moment\"][line]\n",
    "    \n",
    "    PCA_text=PCA_test(array_train_text,array_test_text,4)[line,:]\n",
    "    concat_data[0,8:12]=PCA_text\n",
    "    \n",
    "    PCA_hashtag=PCA_test(array_train_hashtags,array_test_hashtags,3)[line,:]\n",
    "    concat_data[0,12:15]=PCA_hashtag\n",
    "    \n",
    "    return torch.tensor(concat_data).float(), sentence_encoded(df_test, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7284d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_to_test(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e60a6",
   "metadata": {},
   "source": [
    "## <span style=\"color:indigo\">5.2 Predict on the test data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcf9f1",
   "metadata": {},
   "source": [
    "Loss function : Mean Absolute Error (MAE) ->\n",
    "The MAE metric is calculated by dividing the sum of absolute differences between the predicted\n",
    "number of retweets (pi) and the observed number of retweets (ai) by the number of observations\n",
    "(N), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7064857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, predictions):\n",
    "    y_true, predictions = np.array(y_true), np.array(predictions)\n",
    "    return np.mean(np.abs(y_true - predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval=mae(Y,predictions)\n",
    "print(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "68e043ed5fffcd3f0ea568b0d26861e63bcdadeecb0533f1e449c844b9002b2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
